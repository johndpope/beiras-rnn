{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras + tensorflow serving\n",
    "\n",
    "* Convert the keras model to a tf model and save it\n",
    "* Deploy the model with tensorflow-serving\n",
    "* Get sequences from the tensorflow-serving\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the keras model to a tf model and save it\n",
    "\n",
    "Based in [keras-tf](https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../aux/')\n",
    "from beiras_aux import load_coded_dictionaries, predict_next_chars, clean_text\n",
    "from keras.layers import Dense, Activation, GRU\n",
    "from keras.models import Sequential\n",
    "from keras.layers import InputLayer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining the model\n",
    "def create_gru_model( num_chars):\n",
    "    \"\"\"\n",
    "    Define the network\n",
    "    :param\n",
    "        numbers_chars .- Number chars using in the training process\n",
    "    :return:\n",
    "        model .- Model network defined\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    # 1 Layer .- GRU layer 1 should be an GRU module with 200 hidden units\n",
    "    model.add(GRU(200, input_shape=(window_size, num_chars), return_sequences=True))\n",
    "    # 2 Layer .- GRU layer 2 should be an GRU module with 200 hidden units\n",
    "    model.add(GRU(200))\n",
    "    # 2 Layer .-  Dense, with number chars unit and softmax activation\n",
    "    model.add(Dense(num_chars, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important: the model is going to be used as predict, then disable training phase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "K.set_learning_phase(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input size of the network, the entry text must have the same length\n",
    "window_size = 100\n",
    "# Get dictionaries\n",
    "chars_to_indices, indices_to_chars = load_coded_dictionaries()\n",
    "number_chars=len(chars_to_indices)\n",
    "# regenerate the model\n",
    "model=create_gru_model(number_chars)\n",
    "model.load_weights('../model_weights/best_beiras_gru_textdata_weights.hdf5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model as tensorflow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: b'../export-tf/2/saved_model.pb'\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.saved_model import builder as saved_model_builder\n",
    "from tensorflow.python.saved_model import utils\n",
    "from tensorflow.python.saved_model import tag_constants, signature_constants\n",
    "from tensorflow.python.saved_model.signature_def_utils_impl import build_signature_def, predict_signature_def\n",
    "from tensorflow.contrib.session_bundle import exporter\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Path to export, 1 is the version, \n",
    "# we can serve differents version with the same server\n",
    "export_path = \"../export-tf/2\"\n",
    "\n",
    "\n",
    "\n",
    "if os.path.isdir(export_path):\n",
    "    shutil.rmtree(export_path)\n",
    "builder = saved_model_builder.SavedModelBuilder(export_path)\n",
    "\n",
    "signature = predict_signature_def(inputs={'sequence': model.input},\n",
    "                                  outputs={'scores': model.output})\n",
    "\n",
    "with K.get_session() as sess:\n",
    "    builder.add_meta_graph_and_variables(sess=sess,\n",
    "                                         tags=[tag_constants.SERVING],\n",
    "                                         signature_def_map={'serving_default': signature})\n",
    "    builder.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"gru_7_input:0\", shape=(?, 100, 55), dtype=float32)\n",
      "Tensor(\"dense_4/Softmax:0\", shape=(?, 55), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#Input and output shape\n",
    "print(model.input)\n",
    "print(model.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PB file describes the model. \n",
    "If we use builder.save(astext=True), the PB is in text mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model with tensorflow-serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Install \n",
    "\n",
    "```sh \n",
    "echo \"deb [arch=amd64] http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | sudo tee /etc/apt/sources.list.d/tensorflow-serving.list\n",
    "```\n",
    "\n",
    "```sh \n",
    "curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | sudo apt-key add -\n",
    "```\n",
    "\n",
    "\n",
    "```sh \n",
    "sudo apt-get update && sudo apt-get install tensorflow-model-server\n",
    "```\n",
    "\n",
    "**The tensorflow-model-server version do not soport GPU**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lanch server\n",
    "\n",
    "tensorflow_model_server --port=9000 --model_name=default --model_base_path=/home/aind2/beiras-rnn/export-tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get sequences from the tensorflow-serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based in [How to deploy Machine Learning models ](https://medium.com/towards-data-science/how-to-deploy-machine-learning-models-with-tensorflow-part-2-containerize-it-db0ad7ca35a7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In pip repository, there is  tensorflow-serving  API for python2. For python3 we must generate it.**\n",
    "\n",
    "```sh\n",
    "git clone --recurse-submodules https://github.com/tensorflow/serving.git\n",
    "\n",
    "cd <tensorflow serving source folder>\n",
    "# 2\n",
    "mv ./tensorflow ./tensorflow_\n",
    "mv ./tensorflow_/tensorflow .\n",
    "# 3\n",
    "python -m grpc.tools.protoc ./tensorflow_serving/apis/*.proto --python_out=<path to GAN project> --grpc_python_out=<path to GAN project> --proto_path=.\n",
    "# 4\n",
    "mv ./tensorflow ./tensorflow_\n",
    "mv ./tensorflow_ ./tensorflow\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install the pithon grpc**\n",
    "\n",
    "pip install grpcio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'m'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "sys.path.insert(0, '../aux/')\n",
    "from beiras_aux import load_coded_dictionaries, predict_next_chars, clean_text\n",
    "from tensorflow_serving.apis import predict_pb2\n",
    "from tensorflow_serving.apis import prediction_service_pb2\n",
    "from tensorflow_serving.apis import prediction_service_pb2_grpc\n",
    "import tensorflow as tf\n",
    "import grpc\n",
    "\n",
    "input_init=\"se moito cando dixen eu que as suas políticas agresoras do común cidadán matan e a sua cospedal alcu\"\n",
    "#input_init=\"pla panfletaria contra as leoninas taxas impostas polo ministro de xustiza actual malia que vulneran\"\n",
    "# Load values\n",
    "window_size = 100\n",
    "chars_to_indices, indices_to_chars = load_coded_dictionaries()\n",
    "number_chars=len(chars_to_indices)\n",
    "# Clean the text\n",
    "input_clean=clean_text(input_init.lower())\n",
    "input_clean = input_clean[:window_size]\n",
    "# Text to array [1,input_lenght,num_chars]\n",
    "x_test = np.zeros((1,window_size, number_chars))\n",
    "for t, char in enumerate(input_clean):\n",
    "    x_test[0, t, chars_to_indices[char]] = 1.\n",
    "x_test    \n",
    "\n",
    "\n",
    "# Get the array with the probabilities for the next charazter\n",
    "channel = grpc.insecure_channel(\"localhost:\" + str(9000))\n",
    "stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
    "request = predict_pb2.PredictRequest()\n",
    "# Name of the model\n",
    "request.model_spec.name = 'default' \n",
    "request.model_spec.signature_name = 'serving_default' \n",
    "request.inputs['sequence'].CopyFrom( \n",
    "        tf.contrib.util.make_tensor_proto(\n",
    "            x_test,dtype='float32'))\n",
    "result=stub.Predict(request)\n",
    "result.outputs[\"scores\"]   \n",
    "# Get the charazter from array\n",
    "test_predict=np.array(result.outputs[\"scores\"].float_val)\n",
    "r = np.argmax(test_predict)  # predict class of each test input\n",
    "d = indices_to_chars[r]\n",
    "d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function\n",
    "def predict_one(text_predict,stub,window_size,number_chars):\n",
    "    # Convert input sequence to array\n",
    "    x_test = np.zeros((1,window_size, number_chars))\n",
    "    for t, char in enumerate(text_predict):\n",
    "        x_test[0, t, chars_to_indices[char]] = 1.\n",
    "    #Prepare the request\n",
    "    request = predict_pb2.PredictRequest() \n",
    "    request.model_spec.name = 'default' \n",
    "    request.model_spec.signature_name = 'serving_default' \n",
    "    request.inputs['sequence'].CopyFrom( \n",
    "        tf.contrib.util.make_tensor_proto(\n",
    "            x_test,dtype='float32'))\n",
    "    #Made the request\n",
    "    result=stub.Predict(request)\n",
    "    # Convert the request return to a charazter\n",
    "    #print(result)\n",
    "    test_predict=np.array(result.outputs[\"scores\"].float_val)\n",
    "    r = np.argmax(test_predict)  # predict class of each test input\n",
    "    return (indices_to_chars[r])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'m'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the function\n",
    "input_init=\"se moito cando dixen eu que as suas políticas agresoras do común cidadán matan e a sua cospedal alcu\"\n",
    "window_size = 100\n",
    "chars_to_indices, indices_to_chars = load_coded_dictionaries()\n",
    "number_chars=len(chars_to_indices)\n",
    "input_clean=clean_text(input_init.lower())\n",
    "channel = grpc.insecure_channel(\"localhost:\" + str(9000))\n",
    "stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
    "d=predict_one(input_clean,stub,window_size,number_chars)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Complete a sequence using the server\n",
    "def predict_window(text_predict,number_predict,window_size):\n",
    "    # Get dictionaries\n",
    "    chars_to_indices, indices_to_chars = load_coded_dictionaries()\n",
    "    number_chars=len(chars_to_indices)\n",
    "    # Clean the test\n",
    "    input_clean=clean_text(text_predict.lower())\n",
    "    # Get stub\n",
    "    channel = grpc.insecure_channel(\"localhost:\" + str(9000))\n",
    "    stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
    "    # Call server for all charazters\n",
    "    for i in range(number_predict):\n",
    "        d=predict_one(input_clean[i:],stub,window_size,number_chars)\n",
    "        input_clean+=d\n",
    "    return input_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pla panfletaria contra as leoninas taxas impostas polo ministro de xustiza actual malia que vulneran un contrasentido arestora e a construción de anos de autonomía galega non é unha concepción do seu '"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test predict window\n",
    "beiras_said=\"pla panfletaria contra as leoninas taxas impostas polo ministro de xustiza actual malia que vulneran\"\n",
    "#beiras_said=\"se moito cando dixen eu que as suas políticas agresoras do común cidadán matan e a sua cospedal alcu\"\n",
    "text=predict_window(beiras_said,window_size,window_size)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

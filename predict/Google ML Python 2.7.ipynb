{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conda create  -f google-cloud-ml.yml\n",
    "source activate google-cloud-ml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "apt-get install sudo curl and install gcloud\n",
    "\n",
    "https://cloud.google.com/sdk/docs/quickstart-debian-ubuntu\n",
    "# Create an environment variable for the correct distribution\n",
    "export CLOUD_SDK_REPO=\"cloud-sdk-$(lsb_release -c -s)\"\n",
    "\n",
    "# Add the Cloud SDK distribution URI as a package source\n",
    "echo \"deb http://packages.cloud.google.com/apt $CLOUD_SDK_REPO main\" | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list\n",
    "\n",
    "# Import the Google Cloud Platform public key\n",
    "curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
    "\n",
    "# Update the package list and install the Cloud SDK\n",
    "sudo apt-get update && sudo apt-get install google-cloud-sdk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init gcloud ans set project and authenticate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gcloud init\n",
    "cloud auth application-default login\n",
    "gcloud config set project ai-ml-dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recreate the model and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import sys\n",
    "sys.path.insert(0, '../aux/')\n",
    "from beiras_aux import load_coded_dictionaries, predict_next_chars,load_text\n",
    "from keras.layers import Dense, Activation, GRU\n",
    "from keras.models import Sequential\n",
    "from keras.layers import InputLayer\n",
    "import os\n",
    "# Defining the model\n",
    "def create_gru_model( num_chars):\n",
    "    \"\"\"\n",
    "    Define the network\n",
    "    :param\n",
    "        numbers_chars .- Number chars using in the training process\n",
    "    :return:\n",
    "        model .- Model network defined\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    # 1 Layer .- GRU layer 1 should be an GRU module with 200 hidden units\n",
    "    model.add(GRU(200, input_shape=(window_size, num_chars), return_sequences=True))\n",
    "    # 2 Layer .- GRU layer 2 should be an GRU module with 200 hidden units\n",
    "    model.add(GRU(200))\n",
    "    # 2 Layer .-  Dense, with number chars unit and softmax activation\n",
    "    model.add(Dense(num_chars, activation='softmax'))\n",
    "    return model\n",
    "import keras.backend as K\n",
    "K.set_learning_phase(0)\n",
    "# Input size of the network, the entry text must have the same length\n",
    "window_size = 100\n",
    "# Get dictionaries\n",
    "chars_to_indices, indices_to_chars = load_coded_dictionaries()\n",
    "number_chars=len(chars_to_indices)\n",
    "# regenerate the model\n",
    "model=create_gru_model(number_chars)\n",
    "model.load_weights('../model_weights/best_beiras_gru_textdata_weights.hdf5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: ../export-tf/3/saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.saved_model import builder as saved_model_builder\n",
    "from tensorflow.python.saved_model import utils\n",
    "from tensorflow.python.saved_model import tag_constants, signature_constants\n",
    "from tensorflow.python.saved_model.signature_def_utils_impl import build_signature_def, predict_signature_def\n",
    "from tensorflow.contrib.session_bundle import exporter\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Path to export, 1 is the version, \n",
    "# we can serve differents version with the same server\n",
    "export_path = \"../export-tf/3\"\n",
    "\n",
    "\n",
    "\n",
    "if os.path.isdir(export_path):\n",
    "    shutil.rmtree(export_path)\n",
    "builder = saved_model_builder.SavedModelBuilder(export_path)\n",
    "\n",
    "signature = predict_signature_def(inputs={'sequence': model.input},\n",
    "                                  outputs={'scores': model.output})\n",
    "\n",
    "with K.get_session() as sess:\n",
    "    builder.add_meta_graph_and_variables(sess=sess,\n",
    "                                         tags=[tag_constants.SERVING],\n",
    "                                         signature_def_map={'serving_default': signature})\n",
    "    builder.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make local prediccions\n",
    "gcloud ml-engine local predict --model-dir=/home/jota/beiras-rnn/export-tf/3 --json-instances=/home/jota/beiras-rnn/predict/data-ml-local.json > predict/return.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'm'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "y_test=[0]\n",
    "with open('return.json',\"r\") as input:\n",
    "    with open('return2.json',\"w\") as output:\n",
    "        n=0;\n",
    "        for line in input:\n",
    "            if n==1:\n",
    "                output.write(line)\n",
    "            n+=1\n",
    "\n",
    "with open('return2.json', 'r') as fp:\n",
    "    y_test=json.load(fp)\n",
    "test_predict=np.array(y_test)\n",
    "r = np.argmax(test_predict)  # predict class of each test input\n",
    "d=indices_to_chars[r]\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Make bucket and create model\n",
    "MODEL_NAME=BeirasRnn\n",
    "gcloud ml-engine models create $MODEL_NAME --enable-logging\n",
    "gsutil mb gs://beiras_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gsutil cp -r ./export-tf/3 gs://beiras_rnn/export-google-ml/10\n",
    "MODEL_NAME=BeirasRnn\n",
    "DEPLOYMENT_SOURCE=\"gs://beiras_rnn/export-google-ml/10\"\n",
    "gcloud ml-engine versions create \"v10\" --model $MODEL_NAME  --origin $DEPLOYMENT_SOURCE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install --upgrade google-api-python-client"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Predict one charazter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jotavaladouro/anaconda/envs/google-cloud-ml/lib/python2.7/site-packages/ipykernel_launcher.py:20: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n",
      "[  3.51546885e-04   1.14319491e-11   2.63947986e-05   1.84414816e-07\n",
      "   3.36886907e-04   2.11999195e-05   2.33439205e-05   1.53388009e-06\n",
      "   8.02952115e-07   3.70742782e-06   1.38431062e-06   5.31530422e-05\n",
      "   1.34768698e-06   1.37859752e-05   3.21813583e-01   2.69034554e-05\n",
      "   2.03325999e-05   2.56823409e-06   4.12520580e-02   9.74724799e-07\n",
      "   3.45889275e-05   3.13729830e-02   2.14483138e-04   3.60666263e-06\n",
      "   1.53559540e-06   2.02687289e-07   6.53400889e-07   7.27140832e-06\n",
      "   5.08609153e-02   2.23578081e-06   1.06179132e-05   8.22419770e-06\n",
      "   1.15277899e-05   6.45663531e-05   5.44817686e-01   2.26951920e-06\n",
      "   9.90975650e-07   9.07722006e-06   7.59744034e-06   6.03258854e-07\n",
      "   1.23464641e-11   1.07990136e-11   1.55533117e-03   1.19396784e-11\n",
      "   2.84433155e-03   1.18369602e-11   2.25673048e-05   1.17487201e-11\n",
      "   4.03751437e-06   1.10910300e-11   3.98885738e-03   1.20199024e-11\n",
      "   2.01249553e-04   1.92584679e-07   1.13429536e-07]\n",
      "(34, u'u')\n"
     ]
    }
   ],
   "source": [
    "import googleapiclient.discovery\n",
    "import sys\n",
    "import numpy as np\n",
    "sys.path.insert(0, '../aux/')\n",
    "from beiras_aux import load_coded_dictionaries, predict_next_chars, clean_text\n",
    "\n",
    "\n",
    "input_init=\"se moito cando dixen eu que as suas políticas agresoras do común cidadán matan e a sua cospedal alcu\"\n",
    "#input_init=\"pla panfletaria contra as leoninas taxas impostas polo ministro de xustiza actual malia que vulneran\"\n",
    "# Load values\n",
    "window_size = 100\n",
    "chars_to_indices, indices_to_chars = load_coded_dictionaries()\n",
    "number_chars=len(chars_to_indices)\n",
    "# Clean the text\n",
    "input_clean=clean_text(input_init.lower())\n",
    "input_clean = input_clean[:window_size]\n",
    "# Text to array [1,input_lenght,num_chars]\n",
    "x_test = np.zeros((window_size, number_chars))\n",
    "for t, char in enumerate(input_clean):\n",
    "    if char in chars_to_indices:\n",
    "        x_test[ t, chars_to_indices[char]] = 1.\n",
    "#x_test   \n",
    "#x_test = np.zeros((number_chars,window_size))\n",
    "#for t, char in enumerate(input_clean):\n",
    "#    if char in chars_to_indices:\n",
    "#        x_test[ chars_to_indices[char],t] = 1.\n",
    "print(len(x_test[0]))    \n",
    "project=\"ai-ml-dl\"\n",
    "model=\"BeirasRnn\"\n",
    "version=\"v10\"\n",
    "service = googleapiclient.discovery.build('ml', 'v1')\n",
    "name = 'projects/{}/models/{}'.format(project, model)\n",
    "if version is not None:\n",
    "        name += '/versions/{}'.format(version)\n",
    "instances={'sequence':x_test.tolist()}\n",
    "response = service.projects().predict(\n",
    "        name=name,\n",
    "        body={'instances': instances}\n",
    "    ).execute()\n",
    "if 'error' in response:\n",
    "    raise RuntimeError(response['error'])\n",
    "test_predict=np.array(response['predictions'][0]['scores'])\n",
    "r = np.argmax(test_predict)  # predict class of each test input\n",
    "print(test_predict)\n",
    "print(r,indices_to_chars[r])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict sentece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function\n",
    "def predict_one(text_predict,service,model_name,window_size,chars_to_indices, indices_to_chars):\n",
    "    # Convert input sequence to array\n",
    "    number_chars=len(chars_to_indices)\n",
    "    x_test = np.zeros((window_size,number_chars))\n",
    "    for t, char in enumerate(text_predict):\n",
    "       x_test[t,chars_to_indices[char]] = 1.\n",
    "    #print(x_test.shape)\n",
    "    x_test=x_test[:window_size,:]\n",
    "    #Prepare the request\n",
    "    instances={'sequence':x_test.tolist()}\n",
    "    response = service.projects().predict(\n",
    "        name=name,\n",
    "        body={'instances': instances}\n",
    "    ).execute()\n",
    "    if 'error' in response:\n",
    "        raise RuntimeError(response['error'])\n",
    "    test_predict=np.array(response['predictions'][0]['scores'])\n",
    "    r = np.argmax(test_predict)  # predict class of each test input\n",
    "    return (indices_to_chars[r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete a sequence using the server\n",
    "def predict_window(text_predict,number_predict,window_size,lproject,lmodel,lversion):\n",
    "    \n",
    "    # Get dictionaries\n",
    "    chars_to_indices, indices_to_chars = load_coded_dictionaries()\n",
    "    # Clean the test\n",
    "    input_clean=clean_text(text_predict.lower())\n",
    "    # Get stub\n",
    "    service = googleapiclient.discovery.build('ml', 'v1')\n",
    "    name = 'projects/{}/models/{}'.format(lproject, lmodel)\n",
    "    if lversion is not None:\n",
    "        name += '/versions/{}'.format(lversion)\n",
    "    print(name)\n",
    "    # Call server for all charazters\n",
    "    for i in range(number_predict):\n",
    "        d=predict_one(input_clean[i:],service,name,window_size,chars_to_indices, indices_to_chars)\n",
    "        input_clean+=d\n",
    "    return input_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projects/ai-ml-dl/models/BeirasRnn/versions/v10\n",
      "se moito cando dixen eu que as suas políticas agresoras do común cidadán matan e a sua cospedal alcumalo de estado español e o proceso de descomposición do poder constitucional e desembocar nestas asembleas a unha concepción do sistema de estado e o proceso de descomposición do sistema-mundo as cond\n"
     ]
    }
   ],
   "source": [
    "import googleapiclient.discovery\n",
    "project=\"ai-ml-dl\"\n",
    "model=\"BeirasRnn\"\n",
    "version=\"v10\"\n",
    "\n",
    "input_init=u\"se moito cando dixen eu que as suas políticas agresoras do común cidadán matan e a sua cospedal alcu\"\n",
    "#input_init=\"pla panfletaria contra as leoninas taxas impostas polo ministro de xustiza actual malia que vulneran\"\n",
    "\n",
    "window_size = 100\n",
    "number_predict=200\n",
    "      \n",
    "\n",
    "response=predict_window(input_init[:window_size],number_predict,\n",
    "                        window_size,project,model,version)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our original text has 1213561 characters\n"
     ]
    }
   ],
   "source": [
    "# read in the text, transforming everything to lower case\n",
    "text_org = open('Beiras.txt', encoding=\"utf-8\").read().lower()\n",
    "print('our original text has ' + str(len(text_org)) + ' characters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [http://resistiromorir.blogspot.com.es/p/xose-manuel-beiras-discurso-na.html]\n",
      " ben se decatarían xa as suas señorías de que o texto que veño de ler non está tirado de ningunha copla panfletaria contra as leoninas taxas impostas polo ministro de xustiza actual, malia que vulneran frontalmente o dereito dos cidadáns do común a unha xustiza gratuita, e reduce a cinzas a norma contida no artigo 24.1 da constitución, que predica que \"tódalas persoas teñen dereito a obteren a tutela efectiva dos xuíces e tribunais no exercicio dos seus dereitos e intereses lexítimos, sen que, en ningún caso, se poida producir indefensión\". porque resulta evidente que esa reaccionária decisión ministerial incurre na aberración de convirtir o recurso dos cidadáns á xustiza nunha mercaduría, un ben que compre mercar con cartos, so pena de ficar en indefensión.       non, señorías, non. ese texto non procede de ningún panfleto popular anónimo. tampouco é produto da malsán e rebirichada imaxinación do beiras. lonxe diso: ben se decatarían xa as suas cultas señorías de que é a reprodución literal dun punxente poema de rosalía titulado 'a xusticia pola man' e dado á luz no seu libro follas novas. por certo que, se rosalía publicase arestora ese mesmo poema, non sería alcuñada de 'tola', senón se cadra imputada por 'apoloxía do terrorismo'-ou acusada de incitación á sublevación popular e á desorde pública, que todo pode ser nesta especie de 'franquismo sen franco' que estamos a disfroitar.\n",
      " dirán vostedes que iso todo é pasado e non ven a conto hoxe. pois, co seu permiso, discrépolles. para min, o equivalente actual da muller protagonista anónima do poema rosalián, alén de calquera das mulleres maltratadas hoxendía,   tamén leva nome femenino: élles galiza, élles a cidadanía galega do común: roubada, espoliada, asoballada, desprezada, aldraxada, vítima da inmisericorde cobiza da plutocracia financeira e da inxustiza empoleirada no poder político. unha cidadanía inerme, acosada a tódolos níveis e\n"
     ]
    }
   ],
   "source": [
    "print(text_org[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "text_without_source=\"\";\n",
    "regexp=re.compile(r'http')\n",
    "for line in text_org.splitlines():\n",
    "    if not regexp.search(line):\n",
    "        text_without_source= text_without_source + line\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '!', '\"', '$', '%', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '@', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '¡', '¨', 'ª', '«', '\\xad', '´', 'º', '»', '¿', 'à', 'á', 'â', 'ã', 'ä', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'î', 'ï', 'ñ', 'ò', 'ó', 'ô', 'õ', 'ö', 'ú', 'ü', '–', '—', '‘', '’', '“', '”', '•', '…']\n"
     ]
    }
   ],
   "source": [
    "chars=sorted(list(set(text_without_source)))\n",
    "print(chars);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '$', '-', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '¨', '\\xad', 'á', 'ã', 'é', 'ë', 'í', 'î', 'ñ', 'ò', 'ó', 'õ', 'ú', '–', '—']\n",
      "this corpus has 1174380 total number of characters\n",
      "this corpus has 55 unique characters\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text_clean = re.sub('[ºªàâäçèêïìôöü&%@•…«»”“*/!\"(),.:;_¿¡¿‘’´\\[\\]\\']',' ',text_without_source)\n",
    "text_clean = text_clean.replace(\"  \",\" \")\n",
    "chars=sorted(list(set(text_clean )))\n",
    "print(chars);\n",
    "print (\"this corpus has \" +  str(len(text_clean)) + \" total number of characters\")\n",
    "print (\"this corpus has \" +  str(len(chars)) + \" unique characters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ben se decatarían xa as suas señorías de que o texto que veño de ler non está tirado de ningunha copla panfletaria contra as leoninas taxas impostas polo ministro de xustiza actual malia que vulneran frontalmente o dereito dos cidadáns do común a unha xustiza gratuita e reduce a cinzas a norma contida no artigo 24 1 da constitución que predica que tódalas persoas teñen dereito a obteren a tutela efectiva dos xuíces e tribunais no exercicio dos seus dereitos e intereses lexítimos sen que en ningún caso se poida producir indefensión  porque resulta evidente que esa reaccionária decisión ministerial incurre na aberración de convirtir o recurso dos cidadáns á xustiza nunha mercaduría un ben que compre mercar con cartos so pena de ficar en indefensión    non señorías non ese texto non procede de ningún panfleto popular anónimo tampouco é produto da malsán e rebirichada imaxinación do beiras lonxe diso ben se decatarían xa as suas cultas señorías de que é a reprodución literal dun punxente poema de rosalía titulado a xusticia pola man e dado á luz no seu libro follas novas por certo que se rosalía publicase arestora ese mesmo poema non sería alcuñada de tola  senón se cadra imputada por apoloxía do terrorismo -ou acusada de incitación á sublevación popular e á desorde pública que todo pode ser nesta especie de franquismo sen franco que estamos a disfroitar dirán vostedes que iso todo é pasado e non ven a conto hoxe pois co seu permiso discrépolles para min o equivalente actual da muller protagonista anónima do poema rosalián alén de calquera das mulleres maltratadas hoxendía  tamén leva nome femenino élles galiza élles a cidadanía galega do común roubada espoliada asoballada desprezada aldraxada vítima da inmisericorde cobiza da plutocracia financeira e da inxustiza empoleirada no poder político unha cidadanía inerme acosada a tódolos níveis e en tódalas dimensións dos seus dereitos liberdades e condicións materiais de existencia unhas clases asalariadas reconducidas á s\n"
     ]
    }
   ],
   "source": [
    "print(text_clean[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def window_transform_text(text,window_size,step_size):\n",
    "    # containers for input/output pairs\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    #Number of windows to create\n",
    "    n_windows=int((len(text) - window_size)/ step_size)\n",
    "    for j in range(n_windows) :\n",
    "        # k .- Start index\n",
    "        k= j * step_size\n",
    "        inputs.append(text[k:(k+window_size)])\n",
    "        outputs.append(text[k+window_size])\n",
    "\n",
    "    return inputs,outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run your text window-ing function \n",
    "window_size = 100\n",
    "step_size = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this dictionary is a function mapping each unique character to a unique integer\n",
    "chars_to_indices = dict((c, i) for i, c in enumerate(chars))  # map each unique character to unique integer\n",
    "\n",
    "# this dictionary is a function mapping each unique integer back to a unique character\n",
    "indices_to_chars = dict((i, c) for i, c in enumerate(chars))  # map each unique integer back to unique character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform character-based input/output into equivalent numerical versions\n",
    "def encode_io_pairs(text,window_size,step_size):\n",
    "    # number of unique chars\n",
    "    chars = sorted(list(set(text)))\n",
    "    num_chars = len(chars)\n",
    "    \n",
    "    # cut up text into character input/output pairs\n",
    "    inputs, outputs = window_transform_text(text,window_size,step_size)\n",
    "    \n",
    "    # create empty vessels for one-hot encoded input/output\n",
    "    X = np.zeros((len(inputs), window_size, num_chars), dtype=np.bool)\n",
    "    y = np.zeros((len(inputs), num_chars), dtype=np.bool)\n",
    "    \n",
    "    # loop over inputs/outputs and tranform and store in X/y\n",
    "    for i, sentence in enumerate(inputs):\n",
    "        for t, char in enumerate(sentence):\n",
    "            X[i, t, chars_to_indices[char]] = 1\n",
    "        y[i, chars_to_indices[outputs[i]]] = 1\n",
    "        \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use your function\n",
    "X,y = encode_io_pairs(text_clean,window_size,step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "### necessary functions from the keras library\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import keras\n",
    "import random\n",
    "\n",
    "# TODO build the required RNN model: a single LSTM hidden layer with softmax activation, categorical_crossentropy loss \n",
    "#Number of unique chars\n",
    "def create_simple_model(chars):\n",
    "    num_chars = len(chars)\n",
    "    model= Sequential()\n",
    "    # 1 Layer .- LSTM layer 1 should be an LSTM module with 200 hidden units\n",
    "    model.add(LSTM(200,input_shape = (window_size,num_chars)))\n",
    "    # 2 Layer .-  Dense, with number chars unit and softmax activation\n",
    "    model.add(Dense(num_chars,activation='softmax'))\n",
    "    # initialize optimizer\n",
    "    optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "    # compile model --> make sure initialized optimizer and callbacks - as defined above - are used\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/cpu:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 727165307909698738, name: \"/gpu:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 63963136\n",
       " locality {\n",
       "   bus_id: 1\n",
       " }\n",
       " incarnation: 2853273436309102722\n",
       " physical_device_desc: \"device: 0, name: GRID K520, pci bus id: 0000:00:03.0\"]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xsmall = X[:10000,:,:]\n",
    "ysmall = y[:10000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "10000/10000 [==============================] - 8s - loss: 3.0791     \n",
      "Epoch 2/40\n",
      "10000/10000 [==============================] - 6s - loss: 2.8685     \n",
      "Epoch 3/40\n",
      "10000/10000 [==============================] - 6s - loss: 2.8488     \n",
      "Epoch 4/40\n",
      "10000/10000 [==============================] - 6s - loss: 2.8180     \n",
      "Epoch 5/40\n",
      "10000/10000 [==============================] - 6s - loss: 2.7818     \n",
      "Epoch 6/40\n",
      "10000/10000 [==============================] - 6s - loss: 2.7325     \n",
      "Epoch 7/40\n",
      "10000/10000 [==============================] - 6s - loss: 2.6752     \n",
      "Epoch 8/40\n",
      "10000/10000 [==============================] - 6s - loss: 2.6200     \n",
      "Epoch 9/40\n",
      "10000/10000 [==============================] - 6s - loss: 2.5709     \n",
      "Epoch 10/40\n",
      "10000/10000 [==============================] - 6s - loss: 2.5232     \n",
      "Epoch 11/40\n",
      "10000/10000 [==============================] - 6s - loss: 2.4754     \n",
      "Epoch 12/40\n",
      "10000/10000 [==============================] - 6s - loss: 2.4384     \n",
      "Epoch 13/40\n",
      "10000/10000 [==============================] - 6s - loss: 2.4000     \n",
      "Epoch 14/40\n",
      "10000/10000 [==============================] - 6s - loss: 2.3707     \n",
      "Epoch 15/40\n",
      "10000/10000 [==============================] - 6s - loss: 2.3431     \n",
      "Epoch 16/40\n",
      "10000/10000 [==============================] - 6s - loss: 2.3245     \n",
      "Epoch 17/40\n",
      "10000/10000 [==============================] - 6s - loss: 2.3015     \n",
      "Epoch 18/40\n",
      "10000/10000 [==============================] - 6s - loss: 2.2822     \n",
      "Epoch 19/40\n",
      "10000/10000 [==============================] - 6s - loss: 2.2592     \n",
      "Epoch 20/40\n",
      "10000/10000 [==============================] - 6s - loss: 2.2521     \n",
      "Epoch 21/40\n",
      "10000/10000 [==============================] - 6s - loss: 2.2370     \n",
      "Epoch 22/40\n",
      "10000/10000 [==============================] - 6s - loss: 2.2248     \n",
      "Epoch 23/40\n",
      "10000/10000 [==============================] - 6s - loss: 2.2037     \n",
      "Epoch 24/40\n",
      "10000/10000 [==============================] - 6s - loss: 2.1964     \n",
      "Epoch 25/40\n",
      "10000/10000 [==============================] - 6s - loss: 2.1829     \n",
      "Epoch 26/40\n",
      "10000/10000 [==============================] - 6s - loss: 2.1709     \n",
      "Epoch 27/40\n",
      "10000/10000 [==============================] - 6s - loss: 2.1604     \n",
      "Epoch 28/40\n",
      "10000/10000 [==============================] - 6s - loss: 2.1476     \n",
      "Epoch 29/40\n",
      "10000/10000 [==============================] - 6s - loss: 2.1386     \n",
      "Epoch 30/40\n",
      "10000/10000 [==============================] - 6s - loss: 2.1243     \n",
      "Epoch 31/40\n",
      "10000/10000 [==============================] - 6s - loss: 2.1151     \n",
      "Epoch 32/40\n",
      "10000/10000 [==============================] - 6s - loss: 2.1047     \n",
      "Epoch 33/40\n",
      "10000/10000 [==============================] - 6s - loss: 2.0943     \n",
      "Epoch 34/40\n",
      "10000/10000 [==============================] - 6s - loss: 2.0864     \n",
      "Epoch 35/40\n",
      "10000/10000 [==============================] - 6s - loss: 2.0734     \n",
      "Epoch 36/40\n",
      "10000/10000 [==============================] - 6s - loss: 2.0607     \n",
      "Epoch 37/40\n",
      "10000/10000 [==============================] - 6s - loss: 2.0543     \n",
      "Epoch 38/40\n",
      "10000/10000 [==============================] - 6s - loss: 2.0417     \n",
      "Epoch 39/40\n",
      "10000/10000 [==============================] - 6s - loss: 2.0336     \n",
      "Epoch 40/40\n",
      "10000/10000 [==============================] - 6s - loss: 2.0211     \n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "model=create_simple_model(chars)\n",
    "model.fit(Xsmall, ysmall, batch_size=500, epochs=40,verbose = 1)\n",
    "\n",
    "# save weights\n",
    "model.save_weights('model_weights/best_beiras_small_textdata_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function that uses trained model to predict a desired number of future characters\n",
    "def predict_next_chars(model,input_chars,num_to_predict):     \n",
    "    # create output\n",
    "    predicted_chars = ''\n",
    "    for i in range(num_to_predict):\n",
    "        # convert this round's predicted characters to numerical input    \n",
    "        x_test = np.zeros((1, window_size, len(chars)))\n",
    "        for t, char in enumerate(input_chars):\n",
    "            x_test[0, t, chars_to_indices[char]] = 1.\n",
    "\n",
    "        # make this round's prediction\n",
    "        test_predict = model.predict(x_test,verbose = 0)[0]\n",
    "\n",
    "        # translate numerical prediction back to characters\n",
    "        r = np.argmax(test_predict)                           # predict class of each test input\n",
    "        d = indices_to_chars[r] \n",
    "\n",
    "        # update predicted_chars and input\n",
    "        predicted_chars+=d\n",
    "        input_chars+=d\n",
    "        input_chars = input_chars[1:]\n",
    "    return predicted_chars\n",
    "\n",
    "def print_predicctions(model,weights_file):\n",
    "    start_inds = [100,1000,5000]\n",
    "\n",
    "    # load in weights\n",
    "    model.load_weights(weights_file)\n",
    "    for s in start_inds:\n",
    "        start_index = s\n",
    "        input_chars = text_clean[start_index: start_index + window_size]\n",
    "\n",
    "        # use the prediction function\n",
    "        predict_input = predict_next_chars(model,input_chars,num_to_predict = 100)\n",
    "\n",
    "        # print out input characters\n",
    "        print('------------------')\n",
    "        input_line = 'input chars = ' + '\\n' +  input_chars + '\"' + '\\n'\n",
    "        print(input_line)\n",
    "\n",
    "        # print out predicted characters\n",
    "        line = 'predicted chars = ' + '\\n' +  predict_input + '\"' + '\\n'\n",
    "        print(line)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "input chars = \n",
      "pla panfletaria contra as leoninas taxas impostas polo ministro de xustiza actual malia que vulneran\"\n",
      "\n",
      "predicted chars = \n",
      " e pola con nos de conte do se contes e do pore conte do e contera de sertera de pola con e do e con\"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "poema de rosalía titulado a xusticia pola man e dado á luz no seu libro follas novas por certo que s\"\n",
      "\n",
      "predicted chars = \n",
      "e conte do se conte do se conte do se conte do se conte do se conte do se conte do se conte do se co\"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "se moito cando dixen eu que as suas políticas agresoras do común cidadán matan e a sua cospedal alcu\"\n",
      "\n",
      "predicted chars = \n",
      "nta do e tora de a dera a de tora de conte do de conte do se conte do se conte do se conte do se con\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model=create_simple_model(chars)\n",
    "print_predicctions(model,'model_weights/best_beiras_small_textdata_weights.hdf5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aind2/anaconda3/envs/beiras-rnn/lib/python3.6/site-packages/keras/models.py:848: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1174280/1174280 [==============================] - 749s - loss: 2.1500   \n",
      "Epoch 2/30\n",
      "1174280/1174280 [==============================] - 750s - loss: 1.8072   \n",
      "Epoch 3/30\n",
      "1174280/1174280 [==============================] - 750s - loss: 1.6631   \n",
      "Epoch 4/30\n",
      "1174280/1174280 [==============================] - 750s - loss: 1.5755   \n",
      "Epoch 5/30\n",
      "1174280/1174280 [==============================] - 750s - loss: 1.5155   \n",
      "Epoch 6/30\n",
      "1174280/1174280 [==============================] - 750s - loss: 1.4716   \n",
      "Epoch 7/30\n",
      "1174280/1174280 [==============================] - 750s - loss: 1.4418   \n",
      "Epoch 8/30\n",
      "1174280/1174280 [==============================] - 750s - loss: 1.4195   \n",
      "Epoch 9/30\n",
      "1174280/1174280 [==============================] - 750s - loss: 1.3888   \n",
      "Epoch 10/30\n",
      "1174280/1174280 [==============================] - 750s - loss: 1.4128   \n",
      "Epoch 11/30\n",
      "1174280/1174280 [==============================] - 750s - loss: 1.4041   \n",
      "Epoch 12/30\n",
      "1174280/1174280 [==============================] - 750s - loss: 1.3407   \n",
      "Epoch 13/30\n",
      "1174280/1174280 [==============================] - 750s - loss: 1.3285   \n",
      "Epoch 14/30\n",
      "1174280/1174280 [==============================] - 751s - loss: 1.3178   \n",
      "Epoch 15/30\n",
      "1174280/1174280 [==============================] - 755s - loss: 1.3086   \n",
      "Epoch 16/30\n",
      "1174280/1174280 [==============================] - 755s - loss: 1.3022   \n",
      "Epoch 17/30\n",
      "1174280/1174280 [==============================] - 755s - loss: 1.2916   \n",
      "Epoch 18/30\n",
      "1174280/1174280 [==============================] - 756s - loss: 1.2841   \n",
      "Epoch 19/30\n",
      "1174280/1174280 [==============================] - 755s - loss: 1.2770   \n",
      "Epoch 20/30\n",
      "1174280/1174280 [==============================] - 754s - loss: 1.2711   \n",
      "Epoch 21/30\n",
      "1174280/1174280 [==============================] - 751s - loss: 1.2652   \n",
      "Epoch 22/30\n",
      "1174280/1174280 [==============================] - 751s - loss: 1.2600   \n",
      "Epoch 23/30\n",
      "1174280/1174280 [==============================] - 751s - loss: 1.2549   \n",
      "Epoch 24/30\n",
      "1174280/1174280 [==============================] - 751s - loss: 1.2501   \n",
      "Epoch 25/30\n",
      "1174280/1174280 [==============================] - 750s - loss: 1.2457   \n",
      "Epoch 26/30\n",
      "1174280/1174280 [==============================] - 751s - loss: 1.2415   \n",
      "Epoch 27/30\n",
      "1174280/1174280 [==============================] - 752s - loss: 1.2372   \n",
      "Epoch 28/30\n",
      "1174280/1174280 [==============================] - 752s - loss: 1.2333   \n",
      "Epoch 29/30\n",
      "1174280/1174280 [==============================] - 752s - loss: 1.2298   \n",
      "Epoch 30/30\n",
      "1174280/1174280 [==============================] - 753s - loss: 1.2260   \n"
     ]
    }
   ],
   "source": [
    "# TODO: fit to our larger dataset\n",
    "model=create_simple_model(chars)\n",
    "model.fit(X, y, batch_size=500, nb_epoch=30,verbose = 1)\n",
    "\n",
    "# save weights\n",
    "model.save_weights('model_weights/best_beiras_large_textdata_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "input chars = \n",
      "pla panfletaria contra as leoninas taxas impostas polo ministro de xustiza actual malia que vulneran\"\n",
      "\n",
      "predicted chars = \n",
      " estaban a contra de contra de contra de contra de contra de contra de contra de contra de contra de\"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "poema de rosalía titulado a xusticia pola man e dado á luz no seu libro follas novas por certo que s\"\n",
      "\n",
      "predicted chars = \n",
      "e desenvolver a crise de descomposición do seu contra de contra de contra de contra de contra de con\"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "se moito cando dixen eu que as suas políticas agresoras do común cidadán matan e a sua cospedal alcu\"\n",
      "\n",
      "predicted chars = \n",
      "ñadora do seu contra da contradición nacional e a contradición nacional e a contradición nacional e \"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model=create_simple_model(chars)\n",
    "print_predicctions(model,'model_weights/best_beiras_large_textdata_weights.hdf5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_complex_model(chars):\n",
    "    num_chars = len(chars)\n",
    "    model= Sequential()\n",
    "    # 1 Layer .- LSTM layer 1 should be an LSTM module with 200 hidden units\n",
    "    model.add(LSTM(200,input_shape = (window_size,num_chars),return_sequences=True))\n",
    "    # 2 Layer .-  Dense, with number chars unit and softmax activation\n",
    "    model.add(LSTM(200))\n",
    "    model.add(Dense(num_chars,activation='softmax'))\n",
    "    # initialize optimizer\n",
    "    optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "    # compile model --> make sure initialized optimizer and callbacks - as defined above - are used\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer lstm_2: expected ndim=3, found ndim=2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-415461e19df7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TODO: fit to our larger dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_complex_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-52ea5f1d3ec7>\u001b[0m in \u001b[0;36mcreate_complex_model\u001b[0;34m(chars)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_chars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# 2 Layer .-  Dense, with number chars unit and softmax activation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_chars\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# initialize optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/beiras-rnn/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    473\u001b[0m                           output_shapes=[self.outputs[0]._keras_shape])\n\u001b[1;32m    474\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 raise TypeError('All layers in a Sequential model '\n",
      "\u001b[0;32m~/anaconda3/envs/beiras-rnn/lib/python3.6/site-packages/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, **kwargs)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;31m# modify the input spec to include the state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRecurrent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/beiras-rnn/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m                 \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m                 \u001b[0;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m                 \u001b[0;31m# Collect input shapes to build layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/beiras-rnn/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    455\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m                                      str(K.ndim(x)))\n\u001b[0m\u001b[1;32m    458\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m                 \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 is incompatible with layer lstm_2: expected ndim=3, found ndim=2"
     ]
    }
   ],
   "source": [
    "# TODO: fit to our larger dataset\n",
    "model=create_complex_model(chars)\n",
    "model.summary()\n",
    "print(X.shape)\n",
    "model.fit(X, y, batch_size=500, nb_epoch=30,verbose = 1)\n",
    "\n",
    "# save weights\n",
    "model.save_weights('model_weights/best_beiras_complex_textdata_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "input chars = \n",
      "pla panfletaria contra as leoninas taxas impostas polo ministro de xustiza actual malia que vulneran\"\n",
      "\n",
      "predicted chars = \n",
      " con el e máis a sua parte do partido galeguista e a memória de aquil mesmo contro con este senso má\"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "poema de rosalía titulado a xusticia pola man e dado á luz no seu libro follas novas por certo que s\"\n",
      "\n",
      "predicted chars = \n",
      "e acaso por ser o que estaban a algúns dos colexios e máis a mariña de anos antes de morte ao pé do \"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "se moito cando dixen eu que as suas políticas agresoras do común cidadán matan e a sua cospedal alcu\"\n",
      "\n",
      "predicted chars = \n",
      "ñada polo proprio país e a sua propria conciencia social e política- e a construción dun proxecto es\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model=create_complex_model(chars)\n",
    "print_predicctions(model,'model_weights/best_beiras_complex_textdata_weights.hdf5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Activation,GRU\n",
    "def create_gru_model(chars):\n",
    "    num_chars = len(chars)\n",
    "    model= Sequential()\n",
    "    # 1 Layer .- LSTM layer 1 should be an LSTM module with 200 hidden units\n",
    "    model.add(GRU(200,input_shape = (window_size,num_chars),return_sequences=True))\n",
    "    # 2 Layer .-  Dense, with number chars unit and softmax activation\n",
    "    model.add(GRU(200))\n",
    "    model.add(Dense(num_chars,activation='softmax'))\n",
    "    # initialize optimizer\n",
    "    optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "    # compile model --> make sure initialized optimizer and callbacks - as defined above - are used\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_1 (GRU)                  (None, 100, 200)          153600    \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 200)               240600    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 55)                11055     \n",
      "=================================================================\n",
      "Total params: 405,255\n",
      "Trainable params: 405,255\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(1174280, 100, 55)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aind2/anaconda3/envs/beiras-rnn/lib/python3.6/site-packages/keras/models.py:848: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1174280/1174280 [==============================] - 1221s - loss: 1.8849  \n",
      "Epoch 2/30\n",
      "1174280/1174280 [==============================] - 1222s - loss: 1.5179  \n",
      "Epoch 3/30\n",
      "1174280/1174280 [==============================] - 1222s - loss: 1.4071  \n",
      "Epoch 4/30\n",
      "1174280/1174280 [==============================] - 1223s - loss: 1.3475  \n",
      "Epoch 5/30\n",
      "1174280/1174280 [==============================] - 1222s - loss: 1.3076  \n",
      "Epoch 6/30\n",
      "1174280/1174280 [==============================] - 1222s - loss: 1.2778  \n",
      "Epoch 7/30\n",
      "1174280/1174280 [==============================] - 1223s - loss: 1.2535  \n",
      "Epoch 8/30\n",
      "1174280/1174280 [==============================] - 1223s - loss: 1.2331  \n",
      "Epoch 9/30\n",
      "1174280/1174280 [==============================] - 1222s - loss: 1.2152  \n",
      "Epoch 10/30\n",
      "1174280/1174280 [==============================] - 1223s - loss: 1.1999  \n",
      "Epoch 11/30\n",
      "1174280/1174280 [==============================] - 1223s - loss: 1.1863  \n",
      "Epoch 12/30\n",
      "1174280/1174280 [==============================] - 1224s - loss: 1.1741  \n",
      "Epoch 13/30\n",
      "1174280/1174280 [==============================] - 1223s - loss: 1.1634  \n",
      "Epoch 14/30\n",
      "1174280/1174280 [==============================] - 1224s - loss: 1.1531  \n",
      "Epoch 15/30\n",
      "1174280/1174280 [==============================] - 1223s - loss: 1.1440  \n",
      "Epoch 16/30\n",
      "1174280/1174280 [==============================] - 1224s - loss: 1.1357  \n",
      "Epoch 17/30\n",
      "1174280/1174280 [==============================] - 1224s - loss: 1.1281  \n",
      "Epoch 18/30\n",
      "1174280/1174280 [==============================] - 1224s - loss: 1.1212  \n",
      "Epoch 19/30\n",
      "1174280/1174280 [==============================] - 1224s - loss: 1.1148  \n",
      "Epoch 20/30\n",
      "1174280/1174280 [==============================] - 1225s - loss: 1.1088  \n",
      "Epoch 21/30\n",
      "1174280/1174280 [==============================] - 1224s - loss: 1.1036  \n",
      "Epoch 22/30\n",
      "1174280/1174280 [==============================] - 1224s - loss: 1.0980  \n",
      "Epoch 23/30\n",
      "1174280/1174280 [==============================] - 1225s - loss: 1.0935  \n",
      "Epoch 24/30\n",
      "1174280/1174280 [==============================] - 1225s - loss: 1.0888  \n",
      "Epoch 25/30\n",
      "1174280/1174280 [==============================] - 1224s - loss: 1.0850  \n",
      "Epoch 26/30\n",
      "1174280/1174280 [==============================] - 1224s - loss: 1.0805  \n",
      "Epoch 27/30\n",
      "1174280/1174280 [==============================] - 1225s - loss: 1.0772  \n",
      "Epoch 28/30\n",
      "1174280/1174280 [==============================] - 1225s - loss: 1.0735  \n",
      "Epoch 29/30\n",
      "1174280/1174280 [==============================] - 1225s - loss: 1.0705  \n",
      "Epoch 30/30\n",
      "1174280/1174280 [==============================] - 1225s - loss: 1.0673  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "model=create_gru_model(chars)\n",
    "model.summary()\n",
    "print(X.shape)\n",
    "model.fit(X, y, batch_size=500, nb_epoch=30,verbose = 1)\n",
    "\n",
    "# save weights\n",
    "model.save_weights('model_weights/best_beiras_gru_textdata_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "input chars = \n",
      "pla panfletaria contra as leoninas taxas impostas polo ministro de xustiza actual malia que vulneran\"\n",
      "\n",
      "predicted chars = \n",
      " un contrasentido arestora e a construción de anos de autonomía galega non é unha concepción do seu \"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "poema de rosalía titulado a xusticia pola man e dado á luz no seu libro follas novas por certo que s\"\n",
      "\n",
      "predicted chars = \n",
      "e desenvolve o proceso de descomposición do sistema-mundo as condicións de intervención de capital e\"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "se moito cando dixen eu que as suas políticas agresoras do común cidadán matan e a sua cospedal alcu\"\n",
      "\n",
      "predicted chars = \n",
      "malo de estado español e o proceso de descomposición do poder constitucional e desembocar nestas ase\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model=create_gru_model(chars)\n",
    "print_predicctions(model,'model_weights/best_beiras_gru_textdata_weights.hdf5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_len=len(X)\n",
    "len_train=int(total_len * 0.9)\n",
    "X_train=X[:len_train]\n",
    "y_train=y[:len_train]\n",
    "X_validate=X[len_train:]\n",
    "y_validate=y[len_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1056852 samples, validate on 117428 samples\n",
      "Epoch 1/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 2.1634Epoch 00000: val_loss improved from inf to 1.97206, saving model to model_weights/best_beiras_simple_checkpoint_textdata_weights.hdf5\n",
      "1056852/1056852 [==============================] - 723s - loss: 2.1633 - val_loss: 1.9721\n",
      "Epoch 2/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.8114Epoch 00001: val_loss improved from 1.97206 to 1.81125, saving model to model_weights/best_beiras_simple_checkpoint_textdata_weights.hdf5\n",
      "1056852/1056852 [==============================] - 726s - loss: 1.8114 - val_loss: 1.8112\n",
      "Epoch 3/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.6553Epoch 00002: val_loss improved from 1.81125 to 1.72195, saving model to model_weights/best_beiras_simple_checkpoint_textdata_weights.hdf5\n",
      "1056852/1056852 [==============================] - 726s - loss: 1.6554 - val_loss: 1.7219\n",
      "Epoch 4/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.5612Epoch 00003: val_loss improved from 1.72195 to 1.66641, saving model to model_weights/best_beiras_simple_checkpoint_textdata_weights.hdf5\n",
      "1056852/1056852 [==============================] - 727s - loss: 1.5612 - val_loss: 1.6664\n",
      "Epoch 5/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.4525Epoch 00005: val_loss improved from 1.62892 to 1.60768, saving model to model_weights/best_beiras_simple_checkpoint_textdata_weights.hdf5\n",
      "1056852/1056852 [==============================] - 715s - loss: 1.4525 - val_loss: 1.6077\n",
      "Epoch 7/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.4174Epoch 00006: val_loss improved from 1.60768 to 1.58455, saving model to model_weights/best_beiras_simple_checkpoint_textdata_weights.hdf5\n",
      "1056852/1056852 [==============================] - 710s - loss: 1.4174 - val_loss: 1.5846\n",
      "Epoch 8/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.3890Epoch 00007: val_loss improved from 1.58455 to 1.57481, saving model to model_weights/best_beiras_simple_checkpoint_textdata_weights.hdf5\n",
      "1056852/1056852 [==============================] - 710s - loss: 1.3890 - val_loss: 1.5748\n",
      "Epoch 9/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.3658Epoch 00008: val_loss improved from 1.57481 to 1.56436, saving model to model_weights/best_beiras_simple_checkpoint_textdata_weights.hdf5\n",
      "1056852/1056852 [==============================] - 710s - loss: 1.3658 - val_loss: 1.5644\n",
      "Epoch 10/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.3461Epoch 00009: val_loss improved from 1.56436 to 1.55785, saving model to model_weights/best_beiras_simple_checkpoint_textdata_weights.hdf5\n",
      "1056852/1056852 [==============================] - 710s - loss: 1.3462 - val_loss: 1.5578\n",
      "Epoch 11/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.3293Epoch 00010: val_loss improved from 1.55785 to 1.54841, saving model to model_weights/best_beiras_simple_checkpoint_textdata_weights.hdf5\n",
      "1056852/1056852 [==============================] - 711s - loss: 1.3293 - val_loss: 1.5484\n",
      "Epoch 12/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.3144Epoch 00011: val_loss improved from 1.54841 to 1.54534, saving model to model_weights/best_beiras_simple_checkpoint_textdata_weights.hdf5\n",
      "1056852/1056852 [==============================] - 711s - loss: 1.3144 - val_loss: 1.5453\n",
      "Epoch 13/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.3064Epoch 00012: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 710s - loss: 1.3064 - val_loss: 1.5840\n",
      "Epoch 14/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.2941Epoch 00013: val_loss improved from 1.54534 to 1.53935, saving model to model_weights/best_beiras_simple_checkpoint_textdata_weights.hdf5\n",
      "1056852/1056852 [==============================] - 710s - loss: 1.2941 - val_loss: 1.5394\n",
      "Epoch 15/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.2790Epoch 00014: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 709s - loss: 1.2790 - val_loss: 1.5419\n",
      "Epoch 16/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.2694Epoch 00015: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 710s - loss: 1.2694 - val_loss: 1.5463\n",
      "Epoch 17/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.2608Epoch 00016: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 711s - loss: 1.2608 - val_loss: 1.5438\n",
      "Epoch 18/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.2528Epoch 00017: val_loss improved from 1.53935 to 1.53752, saving model to model_weights/best_beiras_simple_checkpoint_textdata_weights.hdf5\n",
      "1056852/1056852 [==============================] - 710s - loss: 1.2527 - val_loss: 1.5375\n",
      "Epoch 19/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.2466Epoch 00018: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 710s - loss: 1.2466 - val_loss: 1.5416\n",
      "Epoch 20/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.2400Epoch 00019: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 710s - loss: 1.2399 - val_loss: 1.5420\n",
      "Epoch 21/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.2330Epoch 00020: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 710s - loss: 1.2330 - val_loss: 1.5475\n",
      "Epoch 22/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.2272Epoch 00021: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 711s - loss: 1.2272 - val_loss: 1.5465\n",
      "Epoch 23/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.2218Epoch 00022: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 711s - loss: 1.2218 - val_loss: 1.5465\n",
      "Epoch 24/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.2160Epoch 00023: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 711s - loss: 1.2160 - val_loss: 1.5494\n",
      "Epoch 25/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.2101Epoch 00024: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 712s - loss: 1.2101 - val_loss: 1.5538\n",
      "Epoch 26/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.2090Epoch 00025: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 711s - loss: 1.2090 - val_loss: 1.5485\n",
      "Epoch 27/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.2033Epoch 00026: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 711s - loss: 1.2033 - val_loss: 1.5502\n",
      "Epoch 28/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.1990Epoch 00027: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 711s - loss: 1.1990 - val_loss: 1.5564\n",
      "Epoch 29/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.1947Epoch 00028: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 711s - loss: 1.1947 - val_loss: 1.5604\n",
      "Epoch 30/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.1909Epoch 00029: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 711s - loss: 1.1909 - val_loss: 1.5626\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6d7d709400>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### necessary functions from the keras library\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import keras\n",
    "import random\n",
    "from keras.callbacks import ModelCheckpoint   \n",
    "\n",
    "# train the model\n",
    "checkpointer = ModelCheckpoint(filepath='model_weights/best_beiras_simple_checkpoint_textdata_weights.hdf5', verbose=1, \n",
    "                               save_best_only=True)\n",
    "# train the model\n",
    "model=create_simple_model(chars)\n",
    "model.fit(X_train, y_train, batch_size=500, epochs=30,\n",
    "           validation_data=(X_validate, y_validate),\n",
    "          callbacks=[checkpointer],verbose = 1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "input chars = \n",
      "pla panfletaria contra as leoninas taxas impostas polo ministro de xustiza actual malia que vulneran\"\n",
      "\n",
      "predicted chars = \n",
      " a sua constitución estaba a partir de política de castelao e a constitución do colonizador e a cons\"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "poema de rosalía titulado a xusticia pola man e dado á luz no seu libro follas novas por certo que s\"\n",
      "\n",
      "predicted chars = \n",
      "e desenvolve a seguida a sua constitución española de compromiso constitucional e a constitución da \"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "se moito cando dixen eu que as suas políticas agresoras do común cidadán matan e a sua cospedal alcu\"\n",
      "\n",
      "predicted chars = \n",
      "ñada de compromiso constitucional e a constitución da constitución do colonizador e a constitución d\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model=create_simple_model(chars)\n",
    "print_predicctions(model,'model_weights/best_beiras_simple_checkpoint_textdata_weights.hdf5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1056852 samples, validate on 117428 samples\n",
      "Epoch 1/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.9031Epoch 00000: val_loss improved from inf to 1.73509, saving model to model_weights/best_beiras_gru_checkpoint_textdata_weights.hdf5\n",
      "1056852/1056852 [==============================] - 1154s - loss: 1.9030 - val_loss: 1.7351\n",
      "Epoch 2/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.5281Epoch 00001: val_loss improved from 1.73509 to 1.59913, saving model to model_weights/best_beiras_gru_checkpoint_textdata_weights.hdf5\n",
      "1056852/1056852 [==============================] - 1160s - loss: 1.5281 - val_loss: 1.5991\n",
      "Epoch 3/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.4146Epoch 00002: val_loss improved from 1.59913 to 1.54313, saving model to model_weights/best_beiras_gru_checkpoint_textdata_weights.hdf5\n",
      "1056852/1056852 [==============================] - 1162s - loss: 1.4146 - val_loss: 1.5431\n",
      "Epoch 4/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.3534Epoch 00003: val_loss improved from 1.54313 to 1.51628, saving model to model_weights/best_beiras_gru_checkpoint_textdata_weights.hdf5\n",
      "1056852/1056852 [==============================] - 1157s - loss: 1.3533 - val_loss: 1.5163\n",
      "Epoch 5/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.3124Epoch 00004: val_loss improved from 1.51628 to 1.50347, saving model to model_weights/best_beiras_gru_checkpoint_textdata_weights.hdf5\n",
      "1056852/1056852 [==============================] - 1147s - loss: 1.3124 - val_loss: 1.5035\n",
      "Epoch 6/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.2810Epoch 00005: val_loss improved from 1.50347 to 1.48883, saving model to model_weights/best_beiras_gru_checkpoint_textdata_weights.hdf5\n",
      "1056852/1056852 [==============================] - 1149s - loss: 1.2810 - val_loss: 1.4888\n",
      "Epoch 7/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.2561Epoch 00006: val_loss improved from 1.48883 to 1.48354, saving model to model_weights/best_beiras_gru_checkpoint_textdata_weights.hdf5\n",
      "1056852/1056852 [==============================] - 1149s - loss: 1.2561 - val_loss: 1.4835\n",
      "Epoch 8/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.2348Epoch 00007: val_loss improved from 1.48354 to 1.48137, saving model to model_weights/best_beiras_gru_checkpoint_textdata_weights.hdf5\n",
      "1056852/1056852 [==============================] - 1150s - loss: 1.2348 - val_loss: 1.4814\n",
      "Epoch 9/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.2162Epoch 00008: val_loss improved from 1.48137 to 1.47849, saving model to model_weights/best_beiras_gru_checkpoint_textdata_weights.hdf5\n",
      "1056852/1056852 [==============================] - 1147s - loss: 1.2162 - val_loss: 1.4785\n",
      "Epoch 10/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.2000Epoch 00009: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 1148s - loss: 1.1999 - val_loss: 1.4793\n",
      "Epoch 11/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.1853Epoch 00010: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 1151s - loss: 1.1854 - val_loss: 1.4854\n",
      "Epoch 12/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.1718Epoch 00011: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 1151s - loss: 1.1717 - val_loss: 1.4818\n",
      "Epoch 13/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.1600Epoch 00012: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 1142s - loss: 1.1600 - val_loss: 1.4847\n",
      "Epoch 14/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.1493Epoch 00013: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 1147s - loss: 1.1493 - val_loss: 1.4872\n",
      "Epoch 15/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.1389Epoch 00014: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 1147s - loss: 1.1389 - val_loss: 1.4960\n",
      "Epoch 16/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.1296Epoch 00015: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 1145s - loss: 1.1296 - val_loss: 1.4932\n",
      "Epoch 17/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.1204Epoch 00016: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 1146s - loss: 1.1204 - val_loss: 1.5034\n",
      "Epoch 18/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.1130Epoch 00017: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 1147s - loss: 1.1130 - val_loss: 1.5061\n",
      "Epoch 19/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.1053Epoch 00018: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 1148s - loss: 1.1053 - val_loss: 1.5155\n",
      "Epoch 20/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.0985Epoch 00019: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 1149s - loss: 1.0985 - val_loss: 1.5182\n",
      "Epoch 21/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.0918Epoch 00020: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 1147s - loss: 1.0918 - val_loss: 1.5246\n",
      "Epoch 22/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.0863Epoch 00021: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 1147s - loss: 1.0863 - val_loss: 1.5300\n",
      "Epoch 23/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.0808Epoch 00022: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 1148s - loss: 1.0807 - val_loss: 1.5338\n",
      "Epoch 24/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.0757Epoch 00023: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 1147s - loss: 1.0757 - val_loss: 1.5361\n",
      "Epoch 25/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.0708Epoch 00024: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 1144s - loss: 1.0708 - val_loss: 1.5425\n",
      "Epoch 26/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.0661Epoch 00025: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 1153s - loss: 1.0661 - val_loss: 1.5464\n",
      "Epoch 27/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.0617Epoch 00026: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 1151s - loss: 1.0617 - val_loss: 1.5420\n",
      "Epoch 28/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.0580Epoch 00027: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 1155s - loss: 1.0580 - val_loss: 1.5504\n",
      "Epoch 29/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.0536Epoch 00028: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 1154s - loss: 1.0536 - val_loss: 1.5600\n",
      "Epoch 30/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.0505Epoch 00029: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 1152s - loss: 1.0505 - val_loss: 1.5528\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6ed032d3c8>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### necessary functions from the keras library\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import keras\n",
    "import random\n",
    "from keras.callbacks import ModelCheckpoint   \n",
    "\n",
    "# train the model\n",
    "checkpointer = ModelCheckpoint(filepath='model_weights/best_beiras_gru_checkpoint_textdata_weights.hdf5', verbose=1, \n",
    "                               save_best_only=True)\n",
    "# train the model\n",
    "model=create_gru_model(chars)\n",
    "model.fit(X_train, y_train, batch_size=500, epochs=30,\n",
    "           validation_data=(X_validate, y_validate),\n",
    "          callbacks=[checkpointer],verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "input chars = \n",
      "pla panfletaria contra as leoninas taxas impostas polo ministro de xustiza actual malia que vulneran\"\n",
      "\n",
      "predicted chars = \n",
      " a sua propria estrutura de constitución e a sua propria estrutura de constitución e a sua propria e\"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "poema de rosalía titulado a xusticia pola man e dado á luz no seu libro follas novas por certo que s\"\n",
      "\n",
      "predicted chars = \n",
      "e acabar en contra da sua propria estrutura de constitución e a sua propria estrutura de constitució\"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "se moito cando dixen eu que as suas políticas agresoras do común cidadán matan e a sua cospedal alcu\"\n",
      "\n",
      "predicted chars = \n",
      "ñada en contra da sua propria estrutura de constitución e a sua propria estrutura de constitución e \"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model=create_gru_model(chars)\n",
    "print_predicctions(model,'model_weights/best_beiras_gru_checkpoint_textdata_weights.hdf5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "O checkpoint non mellora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense, Activation,GRU\n",
    "def create_gru_dropout_model(chars):\n",
    "    num_chars = len(chars)\n",
    "    model= Sequential()\n",
    "    # 1 Layer .- LSTM layer 1 should be an LSTM module with 200 hidden units\n",
    "    model.add(GRU(200,input_shape = (window_size,num_chars),return_sequences=True))\n",
    "    # 2 Layer .-  Dense, with number chars unit and softmax activation\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(GRU(200))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(num_chars,activation='softmax'))\n",
    "    # initialize optimizer\n",
    "    optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "    # compile model --> make sure initialized optimizer and callbacks - as defined above - are used\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_1 (GRU)                  (None, 100, 200)          153600    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 200)          0         \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 200)               240600    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 55)                11055     \n",
      "=================================================================\n",
      "Total params: 405,255\n",
      "Trainable params: 405,255\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aind2/anaconda3/envs/beiras-rnn/lib/python3.6/site-packages/keras/models.py:848: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1174280/1174280 [==============================] - 1264s - loss: 1.9735  \n",
      "Epoch 2/30\n",
      " 458000/1174280 [==========>...................] - ETA: 772s - loss: 1.6839"
     ]
    }
   ],
   "source": [
    "\n",
    "model=create_gru_dropout_model(chars)\n",
    "model.summary()\n",
    "model.fit(X, y, batch_size=500, nb_epoch=30,verbose = 1)\n",
    "\n",
    "# save weights\n",
    "model.save_weights('model_weights/best_beiras_gru_dropout_textdata_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "input chars = \n",
      "pla panfletaria contra as leoninas taxas impostas polo ministro de xustiza actual malia que vulneran\"\n",
      "\n",
      "predicted chars = \n",
      " por caso a contradición e de contradición e de contradición e de contradición e de contradición e d\"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "poema de rosalía titulado a xusticia pola man e dado á luz no seu libro follas novas por certo que s\"\n",
      "\n",
      "predicted chars = \n",
      "e constitue unha constitución de contradicións de competencia e a construción dos cidadáns do común \"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "se moito cando dixen eu que as suas políticas agresoras do común cidadán matan e a sua cospedal alcu\"\n",
      "\n",
      "predicted chars = \n",
      "mante en cartas de compostela a construción dos cidadáns do común de competencia e a construción dos\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model=create_gru_dropout_model(chars)\n",
    "print_predicctions(model,'model_weights/best_beiras_gru_dropout_textdata_weights.hdf5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout empeora "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Todo o texto come a memoria\n",
    "TEXT_TO_USE=100000\n",
    "\n",
    "# transform character-based input/output into equivalent numerical versions\n",
    "def encode_io_pairs_distributedtime(text,window_size,step_size):\n",
    "    # number of unique chars\n",
    "    chars = sorted(list(set(text)))\n",
    "    num_chars = len(chars)\n",
    "    \n",
    "    # cut up text into character input/output pairs\n",
    "    inputs, outputs = window_transform_text(text,window_size,step_size)\n",
    "    \n",
    "    # create empty vessels for one-hot encoded input/output\n",
    "    X = np.zeros((len(inputs), window_size, num_chars), dtype=np.bool)\n",
    "    y = np.zeros((len(inputs), window_size, num_chars), dtype=np.bool)\n",
    "    \n",
    "    # loop over inputs/outputs and tranform and store in X/y\n",
    "    for i, sentence in enumerate(inputs):\n",
    "        for t, char in enumerate(sentence):\n",
    "            X[i, t, chars_to_indices[char]] = 1\n",
    "            if (t>0):\n",
    "                y[i, t-1, chars_to_indices[char]] = 1\n",
    "        y[i, len(sentence)-1, chars_to_indices[outputs[i]]] = 1\n",
    "\n",
    "        \n",
    "    return X,y\n",
    "X_distributedtime,y_distributedtime = encode_io_pairs_distributedtime(text_clean[TEXT_TO_USE:],window_size,step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Dense, Activation,GRU\n",
    "\n",
    "def create_gru_distributed_model(chars):\n",
    "    num_chars = len(chars)\n",
    "    model= Sequential()\n",
    "    # 1 Layer .- LSTM layer 1 should be an LSTM module with 200 hidden units\n",
    "    model.add(GRU(200,input_shape = (None,num_chars),return_sequences=True))\n",
    "    # 2 Layer .-  Dense, with number chars unit and softmax activation\n",
    "    model.add(GRU(200,return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(num_chars)))\n",
    "    model.add(Activation('softmax'))\n",
    "    # initialize optimizer\n",
    "    optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "    # compile model --> make sure initialized optimizer and callbacks - as defined above - are used\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_1 (GRU)                  (None, None, 200)         153600    \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, None, 200)         240600    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, None, 55)          11055     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, None, 55)          0         \n",
      "=================================================================\n",
      "Total params: 405,255\n",
      "Trainable params: 405,255\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aind2/anaconda3/envs/beiras-rnn/lib/python3.6/site-packages/keras/models.py:848: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1074280/1074280 [==============================] - 1229s - loss: 1.6049  \n",
      "Epoch 2/30\n",
      "1074280/1074280 [==============================] - 1227s - loss: 1.1894  \n",
      "Epoch 3/30\n",
      "1074280/1074280 [==============================] - 1227s - loss: 1.0725  \n",
      "Epoch 4/30\n",
      "1074280/1074280 [==============================] - 1227s - loss: 1.0049  \n",
      "Epoch 5/30\n",
      "1074280/1074280 [==============================] - 1227s - loss: 0.9650  \n",
      "Epoch 6/30\n",
      "1074280/1074280 [==============================] - 1227s - loss: 0.9399  \n",
      "Epoch 7/30\n",
      "1074280/1074280 [==============================] - 1227s - loss: 0.9226  \n",
      "Epoch 8/30\n",
      "1074280/1074280 [==============================] - 1227s - loss: 0.9098  \n",
      "Epoch 9/30\n",
      "1074280/1074280 [==============================] - 1227s - loss: 0.8998  \n",
      "Epoch 10/30\n",
      "1074280/1074280 [==============================] - 1227s - loss: 0.8918  \n",
      "Epoch 11/30\n",
      "1074280/1074280 [==============================] - 1228s - loss: 0.8852  \n",
      "Epoch 12/30\n",
      "1074280/1074280 [==============================] - 1227s - loss: 0.8797  \n",
      "Epoch 13/30\n",
      "1074280/1074280 [==============================] - 1227s - loss: 0.8748  \n",
      "Epoch 14/30\n",
      "1074280/1074280 [==============================] - 1227s - loss: 0.8706  \n",
      "Epoch 15/30\n",
      "1074280/1074280 [==============================] - 1227s - loss: 0.8668  \n",
      "Epoch 16/30\n",
      "1074280/1074280 [==============================] - 1227s - loss: 0.8635  \n",
      "Epoch 17/30\n",
      "1074280/1074280 [==============================] - 1227s - loss: 0.8604  \n",
      "Epoch 18/30\n",
      "1074280/1074280 [==============================] - 1227s - loss: 0.8578  \n",
      "Epoch 19/30\n",
      "1074280/1074280 [==============================] - 1227s - loss: 0.8552  \n",
      "Epoch 20/30\n",
      "1074280/1074280 [==============================] - 1227s - loss: 0.8530  \n",
      "Epoch 21/30\n",
      "1074280/1074280 [==============================] - 1227s - loss: 0.8507  \n",
      "Epoch 22/30\n",
      "1074280/1074280 [==============================] - 1227s - loss: 0.8488  \n",
      "Epoch 23/30\n",
      "1074280/1074280 [==============================] - 1227s - loss: 0.8469  \n",
      "Epoch 24/30\n",
      "1074280/1074280 [==============================] - 1228s - loss: 0.8451  \n",
      "Epoch 25/30\n",
      "1074280/1074280 [==============================] - 1227s - loss: 0.8436  \n",
      "Epoch 26/30\n",
      "1074280/1074280 [==============================] - 1227s - loss: 0.8419  \n",
      "Epoch 27/30\n",
      "1074280/1074280 [==============================] - 1228s - loss: 0.8405  \n",
      "Epoch 28/30\n",
      "1074280/1074280 [==============================] - 1227s - loss: 0.8391  \n",
      "Epoch 29/30\n",
      "1074280/1074280 [==============================] - 1227s - loss: 0.8378  \n",
      "Epoch 30/30\n",
      "1074280/1074280 [==============================] - 1227s - loss: 0.8366  \n"
     ]
    }
   ],
   "source": [
    "model=create_gru_distributed_model(chars)\n",
    "model.summary()\n",
    "model.fit(X_distributedtime, y_distributedtime, batch_size=500, nb_epoch=30,verbose = 1)\n",
    "\n",
    "# save weights\n",
    "model.save_weights('model_weights/best_beiras_gru_distributed_textdata_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function that uses trained model to predict a desired number of future characters\n",
    "def predict_next_chars_distributed(model,input_chars,num_to_predict):     \n",
    "    # create output\n",
    "    predicted_chars = ''\n",
    "    for i in range(num_to_predict):\n",
    "        # convert this round's predicted characters to numerical input    \n",
    "        x_test = np.zeros((1, window_size, len(chars)))\n",
    "        for t, char in enumerate(input_chars):\n",
    "            x_test[0, t, chars_to_indices[char]] = 1.\n",
    "\n",
    "        # make this round's prediction\n",
    "        test_predict = model.predict(x_test,verbose = 0)[0][window_size-1]\n",
    "        \n",
    "\n",
    "        # translate numerical prediction back to characters\n",
    "        r = np.argmax(test_predict)                           # predict class of each test input\n",
    "        d = indices_to_chars[r] \n",
    "\n",
    "        # update predicted_chars and input\n",
    "        predicted_chars+=d\n",
    "        input_chars+=d\n",
    "        input_chars = input_chars[1:]\n",
    "    return predicted_chars\n",
    "\n",
    "def print_predicctions_distributed(model,weights_file):\n",
    "    start_inds = [100,1000,5000]\n",
    "\n",
    "    # load in weights\n",
    "    model.load_weights(weights_file)\n",
    "    for s in start_inds:\n",
    "        start_index = s\n",
    "        input_chars = text_clean[start_index: start_index + window_size]\n",
    "\n",
    "        # use the prediction function\n",
    "        predict_input = predict_next_chars_distributed(model,input_chars,num_to_predict = 100)\n",
    "\n",
    "        # print out input characters\n",
    "        print('------------------')\n",
    "        input_line = 'input chars = ' + '\\n' +  input_chars + '\"' + '\\n'\n",
    "        print(input_line)\n",
    "\n",
    "        # print out predicted characters\n",
    "        line = 'predicted chars = ' + '\\n' +  predict_input + '\"' + '\\n'\n",
    "        print(line)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "input chars = \n",
      "pla panfletaria contra as leoninas taxas impostas polo ministro de xustiza actual malia que vulneran\"\n",
      "\n",
      "predicted chars = \n",
      " entencíar nen sequer por parte da miña memoria infantil e a de castelao de anteriores do sistema co\"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "poema de rosalía titulado a xusticia pola man e dado á luz no seu libro follas novas por certo que s\"\n",
      "\n",
      "predicted chars = \n",
      "e entende a sua propria condea para ser consabidos ao cabo para delato e resultado do colonizador e \"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "se moito cando dixen eu que as suas políticas agresoras do común cidadán matan e a sua cospedal alcu\"\n",
      "\n",
      "predicted chars = \n",
      "nda pola sua peripecia liberal de panorama sen deza na barbarie de galiza de anova -sintenciais neol\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model=create_gru_distributed_model(chars)\n",
    "print_predicctions_distributed(model,'model_weights/best_beiras_gru_distributed_textdata_weights.hdf5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Consigue bos resultados con so 100000 carazteres, pero a base de comerse a memoria, na miña maquina aws con 100000 casi se come toda a memoria."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

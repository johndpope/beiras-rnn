{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our original text has 589161 characters\n"
     ]
    }
   ],
   "source": [
    "# read in the text, transforming everything to lower case\n",
    "text_org = open('Beiras.txt').read().lower()\n",
    "print('our original text has ' + str(len(text_org)) + ' characters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [http://resistiromorir.blogspot.com.es/p/xose-manuel-beiras-discurso-na.html]\n",
      " ben se decatarían xa as suas señorías de que o texto que veño de ler non está tirado de ningunha copla panfletaria contra as leoninas taxas impostas polo ministro de xustiza actual, malia que vulneran frontalmente o dereito dos cidadáns do común a unha xustiza gratuita, e reduce a cinzas a norma contida no artigo 24.1 da constitución, que predica que \"tódalas persoas teñen dereito a obteren a tutela efectiva dos xuíces e tribunais no exercicio dos seus dereitos e intereses lexítimos, sen que, en ningún caso, se poida producir indefensión\". porque resulta evidente que esa reaccionária decisión ministerial incurre na aberración de convirtir o recurso dos cidadáns á xustiza nunha mercaduría, un ben que compre mercar con cartos, so pena de ficar en indefensión.       non, señorías, non. ese texto non procede de ningún panfleto popular anónimo. tampouco é produto da malsán e rebirichada imaxinación do beiras. lo\n"
     ]
    }
   ],
   "source": [
    "print(text_org[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "text_without_source=\"\";\n",
    "regexp=re.compile(r'http')\n",
    "for line in text_org.splitlines():\n",
    "    if not regexp.search(line):\n",
    "        text_without_source= text_without_source + line\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '!', '\"', '%', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '@', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '¡', 'ª', '«', '´', 'º', '»', '¿', 'à', 'á', 'â', 'ä', 'ç', 'è', 'é', 'ê', 'ì', 'í', 'ï', 'ñ', 'ó', 'ô', 'ö', 'ú', 'ü', '–', '—', '‘', '’', '“', '”', '•', '…']\n"
     ]
    }
   ],
   "source": [
    "chars=sorted(list(set(text_without_source)))\n",
    "print(chars);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'á', 'é', 'í', 'ñ', 'ó', 'ú']\n",
      "this corpus has 568575 total number of characters\n",
      "this corpus has 44 unique characters\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text_clean = re.sub('[ºªàâäçèêïìôöü]|\\W+',' ',text_without_source)\n",
    "text_clean = text_clean.replace(\"  \",\" \")\n",
    "chars=sorted(list(set(text_clean )))\n",
    "print(chars);\n",
    "print (\"this corpus has \" +  str(len(text_clean)) + \" total number of characters\")\n",
    "print (\"this corpus has \" +  str(len(chars)) + \" unique characters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ben se decatarían xa as suas señorías de que o texto que veño de ler non está tirado de ningunha copla panfletaria contra as leoninas taxas impostas polo ministro de xustiza actual malia que vulneran frontalmente o dereito dos cidadáns do común a unha xustiza gratuita e reduce a cinzas a norma contida no artigo 24 1 da constitución que predica que tódalas persoas teñen dereito a obteren a tutela efectiva dos xuíces e tribunais no exercicio dos seus dereitos e intereses lexítimos sen que en ningún caso se poida producir indefensión porque resulta evidente que esa reaccionária decisión ministerial incurre na aberración de convirtir o recurso dos cidadáns á xustiza nunha mercaduría un ben que compre mercar con cartos so pena de ficar en indefensión non señorías non ese texto non procede de ningún panfleto popular anónimo tampouco é produto da malsán e rebirichada imaxinación do beiras lonxe diso ben se decatarían xa as suas cultas señorías de que é a reprodución literal dun punxente poem\n"
     ]
    }
   ],
   "source": [
    "print(text_clean[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def window_transform_text(text,window_size,step_size):\n",
    "    # containers for input/output pairs\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    #Number of windows to create\n",
    "    n_windows=int((len(text) - window_size)/ step_size)\n",
    "    for j in range(n_windows) :\n",
    "        # k .- Start index\n",
    "        k= j * step_size\n",
    "        inputs.append(text[k:(k+window_size)])\n",
    "        outputs.append(text[k+window_size])\n",
    "\n",
    "    return inputs,outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run your text window-ing function \n",
    "window_size = 100\n",
    "step_size = 5\n",
    "inputs, outputs = window_transform_text(text_clean,window_size,step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = catarían xa as suas señorías de que o texto que veño de ler non está tirado de ningunha copla panfle\n",
      "output = t\n",
      "--------------\n",
      "input = ún caso se poida producir indefensión porque resulta evidente que esa reaccionária decisión minister\n",
      "output = i\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print out a few of the input/output pairs to verify that we've made the right kind of stuff to learn from\n",
    "print('input = ' + inputs[2])\n",
    "print('output = ' + outputs[2])\n",
    "print('--------------')\n",
    "print('input = ' + inputs[100])\n",
    "print('output = ' + outputs[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this dictionary is a function mapping each unique character to a unique integer\n",
    "chars_to_indices = dict((c, i) for i, c in enumerate(chars))  # map each unique character to unique integer\n",
    "\n",
    "# this dictionary is a function mapping each unique integer back to a unique character\n",
    "indices_to_chars = dict((i, c) for i, c in enumerate(chars))  # map each unique integer back to unique character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform character-based input/output into equivalent numerical versions\n",
    "def encode_io_pairs(text,window_size,step_size):\n",
    "    # number of unique chars\n",
    "    chars = sorted(list(set(text)))\n",
    "    num_chars = len(chars)\n",
    "    \n",
    "    # cut up text into character input/output pairs\n",
    "    inputs, outputs = window_transform_text(text,window_size,step_size)\n",
    "    \n",
    "    # create empty vessels for one-hot encoded input/output\n",
    "    X = np.zeros((len(inputs), window_size, num_chars), dtype=np.bool)\n",
    "    y = np.zeros((len(inputs), num_chars), dtype=np.bool)\n",
    "    \n",
    "    # loop over inputs/outputs and tranform and store in X/y\n",
    "    for i, sentence in enumerate(inputs):\n",
    "        for t, char in enumerate(sentence):\n",
    "            X[i, t, chars_to_indices[char]] = 1\n",
    "        y[i, chars_to_indices[outputs[i]]] = 1\n",
    "        \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use your function\n",
    "window_size = 100\n",
    "step_size = 5\n",
    "X,y = encode_io_pairs(text_clean,window_size,step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "### necessary functions from the keras library\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import keras\n",
    "import random\n",
    "\n",
    "# TODO build the required RNN model: a single LSTM hidden layer with softmax activation, categorical_crossentropy loss \n",
    "#Number of unique chars\n",
    "num_chars = len(chars)\n",
    "\n",
    "model= Sequential()\n",
    "# 1 Layer .- LSTM layer 1 should be an LSTM module with 200 hidden units\n",
    "model.add(LSTM(200,input_shape = (window_size,num_chars)))\n",
    "# 2 Layer .-  Dense, with number chars unit and softmax activation\n",
    "model.add(Dense(num_chars,activation='softmax'))\n",
    "\n",
    "\n",
    "# initialize optimizer\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "# compile model --> make sure initialized optimizer and callbacks - as defined above - are used\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/cpu:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 11909648797020002359, name: \"/gpu:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 103809024\n",
       " locality {\n",
       "   bus_id: 1\n",
       " }\n",
       " incarnation: 5733125414780857672\n",
       " physical_device_desc: \"device: 0, name: GRID K520, pci bus id: 0000:00:03.0\"]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xsmall = X[:10000,:,:]\n",
    "ysmall = y[:10000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "10000/10000 [==============================] - 6s - loss: 1.8807     \n",
      "Epoch 2/40\n",
      "10000/10000 [==============================] - 6s - loss: 1.8661     \n",
      "Epoch 3/40\n",
      "10000/10000 [==============================] - 6s - loss: 1.8473     \n",
      "Epoch 4/40\n",
      "10000/10000 [==============================] - 6s - loss: 1.8224     \n",
      "Epoch 5/40\n",
      "10000/10000 [==============================] - 6s - loss: 1.8062     \n",
      "Epoch 6/40\n",
      "10000/10000 [==============================] - 6s - loss: 1.7884     \n",
      "Epoch 7/40\n",
      "10000/10000 [==============================] - 6s - loss: 1.7641     \n",
      "Epoch 8/40\n",
      "10000/10000 [==============================] - 6s - loss: 1.7491     \n",
      "Epoch 9/40\n",
      "10000/10000 [==============================] - 6s - loss: 1.7247     \n",
      "Epoch 10/40\n",
      "10000/10000 [==============================] - 6s - loss: 1.7019     \n",
      "Epoch 11/40\n",
      "10000/10000 [==============================] - 6s - loss: 1.6757     \n",
      "Epoch 12/40\n",
      "10000/10000 [==============================] - 6s - loss: 1.6538     \n",
      "Epoch 13/40\n",
      "10000/10000 [==============================] - 6s - loss: 1.6339     \n",
      "Epoch 14/40\n",
      "10000/10000 [==============================] - 6s - loss: 1.6091     \n",
      "Epoch 15/40\n",
      "10000/10000 [==============================] - 6s - loss: 1.5839     \n",
      "Epoch 16/40\n",
      "10000/10000 [==============================] - 6s - loss: 1.5649     \n",
      "Epoch 17/40\n",
      "10000/10000 [==============================] - 6s - loss: 1.5238     \n",
      "Epoch 18/40\n",
      "10000/10000 [==============================] - 6s - loss: 1.5086     \n",
      "Epoch 19/40\n",
      "10000/10000 [==============================] - 6s - loss: 1.4797     \n",
      "Epoch 20/40\n",
      "10000/10000 [==============================] - 6s - loss: 1.4591     \n",
      "Epoch 21/40\n",
      "10000/10000 [==============================] - 6s - loss: 1.4248     \n",
      "Epoch 22/40\n",
      "10000/10000 [==============================] - 6s - loss: 1.4075     \n",
      "Epoch 23/40\n",
      "10000/10000 [==============================] - 6s - loss: 1.3748     \n",
      "Epoch 24/40\n",
      "10000/10000 [==============================] - 6s - loss: 1.3559     \n",
      "Epoch 25/40\n",
      "10000/10000 [==============================] - 6s - loss: 1.3259     \n",
      "Epoch 26/40\n",
      "10000/10000 [==============================] - 6s - loss: 1.3011     \n",
      "Epoch 27/40\n",
      "10000/10000 [==============================] - 6s - loss: 1.2785     \n",
      "Epoch 28/40\n",
      "10000/10000 [==============================] - 6s - loss: 1.2630     \n",
      "Epoch 29/40\n",
      "10000/10000 [==============================] - 6s - loss: 1.2178     \n",
      "Epoch 30/40\n",
      "10000/10000 [==============================] - 6s - loss: 1.1967     \n",
      "Epoch 31/40\n",
      "10000/10000 [==============================] - 6s - loss: 1.1700     \n",
      "Epoch 32/40\n",
      "10000/10000 [==============================] - 6s - loss: 1.1555     \n",
      "Epoch 33/40\n",
      "10000/10000 [==============================] - 6s - loss: 1.1316     \n",
      "Epoch 34/40\n",
      "10000/10000 [==============================] - 6s - loss: 1.0995     \n",
      "Epoch 35/40\n",
      "10000/10000 [==============================] - 6s - loss: 1.0663     \n",
      "Epoch 36/40\n",
      "10000/10000 [==============================] - 6s - loss: 1.0550     \n",
      "Epoch 37/40\n",
      "10000/10000 [==============================] - 6s - loss: 1.0180     \n",
      "Epoch 38/40\n",
      "10000/10000 [==============================] - 6s - loss: 1.0060     \n",
      "Epoch 39/40\n",
      "10000/10000 [==============================] - 6s - loss: 0.9695     \n",
      "Epoch 40/40\n",
      "10000/10000 [==============================] - 6s - loss: 0.9540     \n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "`save_weights` requires h5py.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-549f3f918057>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# save weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_weights/best_RNN_small_textdata_weights.hdf5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/beiras-rnn/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36msave_weights\u001b[0;34m(self, filepath, overwrite)\u001b[0m\n\u001b[1;32m    723\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mh5py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`save_weights` requires h5py.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m         \u001b[0;31m# If file exists and should not be overwritten:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moverwrite\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: `save_weights` requires h5py."
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "model.fit(Xsmall, ysmall, batch_size=500, epochs=40,verbose = 1)\n",
    "\n",
    "# save weights\n",
    "model.save_weights('model_weights/best_RNN_small_textdata_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a sequence generator with RNN\n",
    "\n",
    "In this notebook, we develog a Recurrent Neural Network (RNN) to create a Galician language sequence generator. In this project, we use text from the Galician politician Beiras. \n",
    "We test diferents RNN network:\n",
    "* LSTM\n",
    "* GRU\n",
    "* GRU + Dropout\n",
    "\n",
    "We also test ModelCheckpoint in training.\n",
    "\n",
    "The best network for this is case is a GRU with 3 layers.\n",
    "\n",
    "This work is based in \n",
    "https://github.com/udacity/aind2-rnn/blob/master/RNN_project.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "First we load the data and preprocess:\n",
    "* Lower\n",
    "* remove lines with http links\n",
    "* remove symbols: '[ºªàâäçèêïìôöü&%@•…«»”“*/!\"(),.:;_¿¡¿‘’´\\[\\]\\']'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../aux/')\n",
    "import numpy as np\n",
    "from beiras_aux import load_text,predict_next_chars,print_predicctions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "window_size = 100\n",
    "step_size = 1\n",
    "X,y,chars,chars_to_indices,indices_to_chars,text_clean=load_text('../data/Beiras.txt',window_size,step_size);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* X .- Array shape (sentences, window_size, num_chars) .- Input for training.\n",
    "* y .- Array shape (sentences, num_chars) .- Output for training.\n",
    "* chars . -Array with chars we have in the clean text\n",
    "* chars_to_indices,indices_to_chars .- dictionaries to convert fron number to char and char to index\n",
    "* text_clean .- All the text clean.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test we have a GPU\n",
    "I used a g2.2xlarge EC2 machine. Without a GPU this is too slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/cpu:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 13559481850202299021, name: \"/gpu:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 28573696\n",
       " locality {\n",
       "   bus_id: 1\n",
       " }\n",
       " incarnation: 16123509706186973612\n",
       " physical_device_desc: \"device: 0, name: GRID K520, pci bus id: 0000:00:03.0\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple model\n",
    "* LSTM(200)\n",
    "* Dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### necessary functions from the keras library\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "\n",
    "# TODO build the required RNN model: a single LSTM hidden layer with softmax activation, categorical_crossentropy loss \n",
    "#Number of unique chars\n",
    "def create_simple_model(chars):\n",
    "    num_chars = len(chars)\n",
    "    model= Sequential()\n",
    "    # 1 Layer .- LSTM layer 1 should be an LSTM module with 200 hidden units\n",
    "    model.add(LSTM(200,input_shape = (window_size,num_chars)))\n",
    "    # 2 Layer .-  Dense, with number chars unit and softmax activation\n",
    "    model.add(Dense(num_chars,activation='softmax'))\n",
    "    # initialize optimizer\n",
    "    optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "    # compile model --> make sure initialized optimizer and callbacks - as defined above - are used\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train in a small dataset to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xsmall = X[:10000,:,:]\n",
    "ysmall = y[:10000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "model=create_simple_model(chars)\n",
    "model.fit(Xsmall, ysmall, batch_size=500, epochs=40,verbose = 1)\n",
    "\n",
    "# save weights\n",
    "model.save_weights('../model_weights/best_beiras_small_textdata_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aind2/anaconda3/envs/beiras-rnn/lib/python3.6/site-packages/keras/models.py:848: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1174280/1174280 [==============================] - 749s - loss: 2.1500   \n",
      "Epoch 2/30\n",
      "1174280/1174280 [==============================] - 750s - loss: 1.8072   \n",
      "Epoch 3/30\n",
      "1174280/1174280 [==============================] - 750s - loss: 1.6631   \n",
      "Epoch 4/30\n",
      "1174280/1174280 [==============================] - 750s - loss: 1.5755   \n",
      "Epoch 5/30\n",
      "1174280/1174280 [==============================] - 750s - loss: 1.5155   \n",
      "Epoch 6/30\n",
      "1174280/1174280 [==============================] - 750s - loss: 1.4716   \n",
      "Epoch 7/30\n",
      "1174280/1174280 [==============================] - 750s - loss: 1.4418   \n",
      "Epoch 8/30\n",
      "1174280/1174280 [==============================] - 750s - loss: 1.4195   \n",
      "Epoch 9/30\n",
      "1174280/1174280 [==============================] - 750s - loss: 1.3888   \n",
      "Epoch 10/30\n",
      "1174280/1174280 [==============================] - 750s - loss: 1.4128   \n",
      "Epoch 11/30\n",
      "1174280/1174280 [==============================] - 750s - loss: 1.4041   \n",
      "Epoch 12/30\n",
      "1174280/1174280 [==============================] - 750s - loss: 1.3407   \n",
      "Epoch 13/30\n",
      "1174280/1174280 [==============================] - 750s - loss: 1.3285   \n",
      "Epoch 14/30\n",
      "1174280/1174280 [==============================] - 751s - loss: 1.3178   \n",
      "Epoch 15/30\n",
      "1174280/1174280 [==============================] - 755s - loss: 1.3086   \n",
      "Epoch 16/30\n",
      "1174280/1174280 [==============================] - 755s - loss: 1.3022   \n",
      "Epoch 17/30\n",
      "1174280/1174280 [==============================] - 755s - loss: 1.2916   \n",
      "Epoch 18/30\n",
      "1174280/1174280 [==============================] - 756s - loss: 1.2841   \n",
      "Epoch 19/30\n",
      "1174280/1174280 [==============================] - 755s - loss: 1.2770   \n",
      "Epoch 20/30\n",
      "1174280/1174280 [==============================] - 754s - loss: 1.2711   \n",
      "Epoch 21/30\n",
      "1174280/1174280 [==============================] - 751s - loss: 1.2652   \n",
      "Epoch 22/30\n",
      "1174280/1174280 [==============================] - 751s - loss: 1.2600   \n",
      "Epoch 23/30\n",
      "1174280/1174280 [==============================] - 751s - loss: 1.2549   \n",
      "Epoch 24/30\n",
      "1174280/1174280 [==============================] - 751s - loss: 1.2501   \n",
      "Epoch 25/30\n",
      "1174280/1174280 [==============================] - 750s - loss: 1.2457   \n",
      "Epoch 26/30\n",
      "1174280/1174280 [==============================] - 751s - loss: 1.2415   \n",
      "Epoch 27/30\n",
      "1174280/1174280 [==============================] - 752s - loss: 1.2372   \n",
      "Epoch 28/30\n",
      "1174280/1174280 [==============================] - 752s - loss: 1.2333   \n",
      "Epoch 29/30\n",
      "1174280/1174280 [==============================] - 752s - loss: 1.2298   \n",
      "Epoch 30/30\n",
      "1174280/1174280 [==============================] - 753s - loss: 1.2260   \n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Train\n",
    "model=create_simple_model(chars)\n",
    "model.fit(X, y, batch_size=500, nb_epoch=30,verbose = 1)\n",
    "\n",
    "# save weights\n",
    "model.save_weights('../model_weights/best_beiras_large_textdata_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "input chars = \n",
      "pla panfletaria contra as leoninas taxas impostas polo ministro de xustiza actual malia que vulneran\"\n",
      "\n",
      "predicted chars = \n",
      " estaban a contra de contra de contra de contra de contra de contra de contra de contra de contra de\"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "poema de rosalía titulado a xusticia pola man e dado á luz no seu libro follas novas por certo que s\"\n",
      "\n",
      "predicted chars = \n",
      "e desenvolver a crise de descomposición do seu contra de contra de contra de contra de contra de con\"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "se moito cando dixen eu que as suas políticas agresoras do común cidadán matan e a sua cospedal alcu\"\n",
      "\n",
      "predicted chars = \n",
      "ñadora do seu contra da contradición nacional e a contradición nacional e a contradición nacional e \"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Print predicctions\n",
    "model=create_simple_model(chars)\n",
    "print_predicctions(model,'../model_weights/best_beiras_large_textdata_weights.hdf5'\n",
    "                   ,chars_to_indices,indices_to_chars,text_clean,window_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex network\n",
    "* LSTM(200)\n",
    "* LSTM(200)\n",
    "* Dense\n",
    "It is better than simple one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_complex_model(chars):\n",
    "    num_chars = len(chars)\n",
    "    model= Sequential()\n",
    "    # 1 Layer .- LSTM layer 1 should be an LSTM module with 200 hidden units\n",
    "    model.add(LSTM(200,input_shape = (window_size,num_chars),return_sequences=True))\n",
    "    # 2 Layer .- LSTM layer 2 should be an LSTM module with 200 hidden units\n",
    "    model.add(LSTM(200))\n",
    "    # 3 Layer .-  Dense, with number chars unit and softmax activation\n",
    "    model.add(Dense(num_chars,activation='softmax'))\n",
    "    # initialize optimizer\n",
    "    optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "    # compile model --> make sure initialized optimizer and callbacks - as defined above - are used\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "model=create_complex_model(chars)\n",
    "model.summary()\n",
    "print(X.shape)\n",
    "model.fit(X, y, batch_size=500, nb_epoch=30,verbose = 1)\n",
    "\n",
    "# save weights\n",
    "model.save_weights('../model_weights/best_beiras_complex_textdata_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "input chars = \n",
      "pla panfletaria contra as leoninas taxas impostas polo ministro de xustiza actual malia que vulneran\"\n",
      "\n",
      "predicted chars = \n",
      " con el e máis a sua parte do partido galeguista e a memória de aquil mesmo contro con este senso má\"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "poema de rosalía titulado a xusticia pola man e dado á luz no seu libro follas novas por certo que s\"\n",
      "\n",
      "predicted chars = \n",
      "e acaso por ser o que estaban a algúns dos colexios e máis a mariña de anos antes de morte ao pé do \"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "se moito cando dixen eu que as suas políticas agresoras do común cidadán matan e a sua cospedal alcu\"\n",
      "\n",
      "predicted chars = \n",
      "ñada polo proprio país e a sua propria conciencia social e política- e a construción dun proxecto es\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print prediccions.\n",
    "model=create_complex_model(chars)\n",
    "print_predicctions(model,'../model_weights/best_beiras_complex_textdata_weights.hdf5'\n",
    "                   ,chars_to_indices,indices_to_chars,text_clean,window_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex network with GRU\n",
    "* GRU(200)\n",
    "* GRU(200)\n",
    "* Dense\n",
    "\n",
    "It is better than LSTM network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Activation,GRU\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "def create_gru_model(chars):\n",
    "    num_chars = len(chars)\n",
    "    model= Sequential()\n",
    "    # 1 Layer .- GRU layer 1 should be an GRU module with 200 hidden units\n",
    "    model.add(GRU(200,input_shape = (window_size,num_chars),return_sequences=True))\n",
    "    # 2 Layer .- GRU layer 3 should be an GRU module with 200 hidden units\n",
    "    model.add(GRU(200))\n",
    "     # 3 Layer .-  Dense, with number chars unit and softmax activation\n",
    "    model.add(Dense(num_chars,activation='softmax'))\n",
    "    # initialize optimizer\n",
    "    optimizer =RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "    # compile model --> make sure initialized optimizer and callbacks - as defined above - are used\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_1 (GRU)                  (None, 100, 200)          153600    \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 200)               240600    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 55)                11055     \n",
      "=================================================================\n",
      "Total params: 405,255\n",
      "Trainable params: 405,255\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(1174280, 100, 55)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aind2/anaconda3/envs/beiras-rnn/lib/python3.6/site-packages/keras/models.py:848: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1174280/1174280 [==============================] - 1221s - loss: 1.8849  \n",
      "Epoch 2/30\n",
      "1174280/1174280 [==============================] - 1222s - loss: 1.5179  \n",
      "Epoch 3/30\n",
      "1174280/1174280 [==============================] - 1222s - loss: 1.4071  \n",
      "Epoch 4/30\n",
      "1174280/1174280 [==============================] - 1223s - loss: 1.3475  \n",
      "Epoch 5/30\n",
      "1174280/1174280 [==============================] - 1222s - loss: 1.3076  \n",
      "Epoch 6/30\n",
      "1174280/1174280 [==============================] - 1222s - loss: 1.2778  \n",
      "Epoch 7/30\n",
      "1174280/1174280 [==============================] - 1223s - loss: 1.2535  \n",
      "Epoch 8/30\n",
      "1174280/1174280 [==============================] - 1223s - loss: 1.2331  \n",
      "Epoch 9/30\n",
      "1174280/1174280 [==============================] - 1222s - loss: 1.2152  \n",
      "Epoch 10/30\n",
      "1174280/1174280 [==============================] - 1223s - loss: 1.1999  \n",
      "Epoch 11/30\n",
      "1174280/1174280 [==============================] - 1223s - loss: 1.1863  \n",
      "Epoch 12/30\n",
      "1174280/1174280 [==============================] - 1224s - loss: 1.1741  \n",
      "Epoch 13/30\n",
      "1174280/1174280 [==============================] - 1223s - loss: 1.1634  \n",
      "Epoch 14/30\n",
      "1174280/1174280 [==============================] - 1224s - loss: 1.1531  \n",
      "Epoch 15/30\n",
      "1174280/1174280 [==============================] - 1223s - loss: 1.1440  \n",
      "Epoch 16/30\n",
      "1174280/1174280 [==============================] - 1224s - loss: 1.1357  \n",
      "Epoch 17/30\n",
      "1174280/1174280 [==============================] - 1224s - loss: 1.1281  \n",
      "Epoch 18/30\n",
      "1174280/1174280 [==============================] - 1224s - loss: 1.1212  \n",
      "Epoch 19/30\n",
      "1174280/1174280 [==============================] - 1224s - loss: 1.1148  \n",
      "Epoch 20/30\n",
      "1174280/1174280 [==============================] - 1225s - loss: 1.1088  \n",
      "Epoch 21/30\n",
      "1174280/1174280 [==============================] - 1224s - loss: 1.1036  \n",
      "Epoch 22/30\n",
      "1174280/1174280 [==============================] - 1224s - loss: 1.0980  \n",
      "Epoch 23/30\n",
      "1174280/1174280 [==============================] - 1225s - loss: 1.0935  \n",
      "Epoch 24/30\n",
      "1174280/1174280 [==============================] - 1225s - loss: 1.0888  \n",
      "Epoch 25/30\n",
      "1174280/1174280 [==============================] - 1224s - loss: 1.0850  \n",
      "Epoch 26/30\n",
      "1174280/1174280 [==============================] - 1224s - loss: 1.0805  \n",
      "Epoch 27/30\n",
      "1174280/1174280 [==============================] - 1225s - loss: 1.0772  \n",
      "Epoch 28/30\n",
      "1174280/1174280 [==============================] - 1225s - loss: 1.0735  \n",
      "Epoch 29/30\n",
      "1174280/1174280 [==============================] - 1225s - loss: 1.0705  \n",
      "Epoch 30/30\n",
      "1174280/1174280 [==============================] - 1225s - loss: 1.0673  \n"
     ]
    }
   ],
   "source": [
    "#Train\n",
    "model=create_gru_model(chars)\n",
    "model.summary()\n",
    "print(X.shape)\n",
    "model.fit(X, y, batch_size=500, nb_epoch=30,verbose = 1)\n",
    "\n",
    "# save weights\n",
    "model.save_weights('../model_weights/best_beiras_gru_textdata_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pla panfletaria contra as leoninas taxas impostas polo ministro de xustiza actual malia que vulneran.... un contrasentido arestora e a construción de anos de autonomía galega non é unha concepción do seu \n",
      "poema de rosalía titulado a xusticia pola man e dado á luz no seu libro follas novas por certo que s....e desenvolve o proceso de descomposición do sistema-mundo as condicións de intervención de capital e\n",
      "se moito cando dixen eu que as suas políticas agresoras do común cidadán matan e a sua cospedal alcu....malo de estado español e o proceso de descomposición do poder constitucional e desembocar nestas ase\n"
     ]
    }
   ],
   "source": [
    "#Print predicctions\n",
    "model=create_gru_model(chars)\n",
    "print_predicctions(model,'../model_weights/best_beiras_gru_textdata_weights.hdf5' ,\n",
    "                   chars_to_indices,indices_to_chars,text_clean,window_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the  model with ModelCheckpoint\n",
    "Save the best model after every epoch. For select the best model, we create a validate data set.This is a regularization type. \n",
    "**It does not work in this case.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create to dataset: 1 for train, (90%) and other for validate the model.\n",
    "# The validate dataset is used after evety epoch to select the best model\n",
    "\n",
    "total_len=len(X)\n",
    "len_train=int(total_len * 0.9)\n",
    "X_train=X[:len_train]\n",
    "y_train=y[:len_train]\n",
    "X_validate=X[len_train:]\n",
    "y_validate=y[len_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple model  with ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1056852 samples, validate on 117428 samples\n",
      "Epoch 1/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 2.1634Epoch 00000: val_loss improved from inf to 1.97206, saving model to model_weights/best_beiras_simple_checkpoint_textdata_weights.hdf5\n",
      "1056852/1056852 [==============================] - 723s - loss: 2.1633 - val_loss: 1.9721\n",
      "Epoch 2/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.8114Epoch 00001: val_loss improved from 1.97206 to 1.81125, saving model to model_weights/best_beiras_simple_checkpoint_textdata_weights.hdf5\n",
      "1056852/1056852 [==============================] - 726s - loss: 1.8114 - val_loss: 1.8112\n",
      "Epoch 3/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.6553Epoch 00002: val_loss improved from 1.81125 to 1.72195, saving model to model_weights/best_beiras_simple_checkpoint_textdata_weights.hdf5\n",
      "1056852/1056852 [==============================] - 726s - loss: 1.6554 - val_loss: 1.7219\n",
      "Epoch 4/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.5612Epoch 00003: val_loss improved from 1.72195 to 1.66641, saving model to model_weights/best_beiras_simple_checkpoint_textdata_weights.hdf5\n",
      "1056852/1056852 [==============================] - 727s - loss: 1.5612 - val_loss: 1.6664\n",
      "Epoch 5/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.4525Epoch 00005: val_loss improved from 1.62892 to 1.60768, saving model to model_weights/best_beiras_simple_checkpoint_textdata_weights.hdf5\n",
      "1056852/1056852 [==============================] - 715s - loss: 1.4525 - val_loss: 1.6077\n",
      "Epoch 7/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.4174Epoch 00006: val_loss improved from 1.60768 to 1.58455, saving model to model_weights/best_beiras_simple_checkpoint_textdata_weights.hdf5\n",
      "1056852/1056852 [==============================] - 710s - loss: 1.4174 - val_loss: 1.5846\n",
      "Epoch 8/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.3890Epoch 00007: val_loss improved from 1.58455 to 1.57481, saving model to model_weights/best_beiras_simple_checkpoint_textdata_weights.hdf5\n",
      "1056852/1056852 [==============================] - 710s - loss: 1.3890 - val_loss: 1.5748\n",
      "Epoch 9/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.3658Epoch 00008: val_loss improved from 1.57481 to 1.56436, saving model to model_weights/best_beiras_simple_checkpoint_textdata_weights.hdf5\n",
      "1056852/1056852 [==============================] - 710s - loss: 1.3658 - val_loss: 1.5644\n",
      "Epoch 10/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.3461Epoch 00009: val_loss improved from 1.56436 to 1.55785, saving model to model_weights/best_beiras_simple_checkpoint_textdata_weights.hdf5\n",
      "1056852/1056852 [==============================] - 710s - loss: 1.3462 - val_loss: 1.5578\n",
      "Epoch 11/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.3293Epoch 00010: val_loss improved from 1.55785 to 1.54841, saving model to model_weights/best_beiras_simple_checkpoint_textdata_weights.hdf5\n",
      "1056852/1056852 [==============================] - 711s - loss: 1.3293 - val_loss: 1.5484\n",
      "Epoch 12/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.3144Epoch 00011: val_loss improved from 1.54841 to 1.54534, saving model to model_weights/best_beiras_simple_checkpoint_textdata_weights.hdf5\n",
      "1056852/1056852 [==============================] - 711s - loss: 1.3144 - val_loss: 1.5453\n",
      "Epoch 13/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.3064Epoch 00012: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 710s - loss: 1.3064 - val_loss: 1.5840\n",
      "Epoch 14/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.2941Epoch 00013: val_loss improved from 1.54534 to 1.53935, saving model to model_weights/best_beiras_simple_checkpoint_textdata_weights.hdf5\n",
      "1056852/1056852 [==============================] - 710s - loss: 1.2941 - val_loss: 1.5394\n",
      "Epoch 15/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.2790Epoch 00014: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 709s - loss: 1.2790 - val_loss: 1.5419\n",
      "Epoch 16/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.2694Epoch 00015: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 710s - loss: 1.2694 - val_loss: 1.5463\n",
      "Epoch 17/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.2608Epoch 00016: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 711s - loss: 1.2608 - val_loss: 1.5438\n",
      "Epoch 18/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.2528Epoch 00017: val_loss improved from 1.53935 to 1.53752, saving model to model_weights/best_beiras_simple_checkpoint_textdata_weights.hdf5\n",
      "1056852/1056852 [==============================] - 710s - loss: 1.2527 - val_loss: 1.5375\n",
      "Epoch 19/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.2466Epoch 00018: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 710s - loss: 1.2466 - val_loss: 1.5416\n",
      "Epoch 20/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.2400Epoch 00019: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 710s - loss: 1.2399 - val_loss: 1.5420\n",
      "Epoch 21/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.2330Epoch 00020: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 710s - loss: 1.2330 - val_loss: 1.5475\n",
      "Epoch 22/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.2272Epoch 00021: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 711s - loss: 1.2272 - val_loss: 1.5465\n",
      "Epoch 23/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.2218Epoch 00022: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 711s - loss: 1.2218 - val_loss: 1.5465\n",
      "Epoch 24/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.2160Epoch 00023: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 711s - loss: 1.2160 - val_loss: 1.5494\n",
      "Epoch 25/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.2101Epoch 00024: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 712s - loss: 1.2101 - val_loss: 1.5538\n",
      "Epoch 26/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.2090Epoch 00025: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 711s - loss: 1.2090 - val_loss: 1.5485\n",
      "Epoch 27/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.2033Epoch 00026: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 711s - loss: 1.2033 - val_loss: 1.5502\n",
      "Epoch 28/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.1990Epoch 00027: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 711s - loss: 1.1990 - val_loss: 1.5564\n",
      "Epoch 29/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.1947Epoch 00028: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 711s - loss: 1.1947 - val_loss: 1.5604\n",
      "Epoch 30/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.1909Epoch 00029: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 711s - loss: 1.1909 - val_loss: 1.5626\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6d7d709400>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import keras\n",
    "import random\n",
    "from keras.callbacks import ModelCheckpoint   \n",
    "\n",
    "# train the model\n",
    "checkpointer = ModelCheckpoint(filepath='../model_weights/best_beiras_simple_checkpoint_textdata_weights.hdf5', verbose=1, \n",
    "                               save_best_only=True)\n",
    "# train the model\n",
    "model=create_simple_model(chars)\n",
    "model.fit(X_train, y_train, batch_size=500, epochs=30,\n",
    "           validation_data=(X_validate, y_validate),\n",
    "          callbacks=[checkpointer],verbose = 1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "input chars = \n",
      "pla panfletaria contra as leoninas taxas impostas polo ministro de xustiza actual malia que vulneran\"\n",
      "\n",
      "predicted chars = \n",
      " a sua constitución estaba a partir de política de castelao e a constitución do colonizador e a cons\"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "poema de rosalía titulado a xusticia pola man e dado á luz no seu libro follas novas por certo que s\"\n",
      "\n",
      "predicted chars = \n",
      "e desenvolve a seguida a sua constitución española de compromiso constitucional e a constitución da \"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "se moito cando dixen eu que as suas políticas agresoras do común cidadán matan e a sua cospedal alcu\"\n",
      "\n",
      "predicted chars = \n",
      "ñada de compromiso constitucional e a constitución da constitución do colonizador e a constitución d\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model=create_simple_model(chars)\n",
    "print_predicctions(model,'../model_weights/best_beiras_simple_checkpoint_textdata_weights.hdf5',\n",
    "                   chars_to_indices,indices_to_chars,text_clean,window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gru model  with ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1056852 samples, validate on 117428 samples\n",
      "Epoch 1/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.9031Epoch 00000: val_loss improved from inf to 1.73509, saving model to model_weights/best_beiras_gru_checkpoint_textdata_weights.hdf5\n",
      "1056852/1056852 [==============================] - 1154s - loss: 1.9030 - val_loss: 1.7351\n",
      "Epoch 2/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.5281Epoch 00001: val_loss improved from 1.73509 to 1.59913, saving model to model_weights/best_beiras_gru_checkpoint_textdata_weights.hdf5\n",
      "1056852/1056852 [==============================] - 1160s - loss: 1.5281 - val_loss: 1.5991\n",
      "Epoch 3/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.4146Epoch 00002: val_loss improved from 1.59913 to 1.54313, saving model to model_weights/best_beiras_gru_checkpoint_textdata_weights.hdf5\n",
      "1056852/1056852 [==============================] - 1162s - loss: 1.4146 - val_loss: 1.5431\n",
      "Epoch 4/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.3534Epoch 00003: val_loss improved from 1.54313 to 1.51628, saving model to model_weights/best_beiras_gru_checkpoint_textdata_weights.hdf5\n",
      "1056852/1056852 [==============================] - 1157s - loss: 1.3533 - val_loss: 1.5163\n",
      "Epoch 5/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.3124Epoch 00004: val_loss improved from 1.51628 to 1.50347, saving model to model_weights/best_beiras_gru_checkpoint_textdata_weights.hdf5\n",
      "1056852/1056852 [==============================] - 1147s - loss: 1.3124 - val_loss: 1.5035\n",
      "Epoch 6/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.2810Epoch 00005: val_loss improved from 1.50347 to 1.48883, saving model to model_weights/best_beiras_gru_checkpoint_textdata_weights.hdf5\n",
      "1056852/1056852 [==============================] - 1149s - loss: 1.2810 - val_loss: 1.4888\n",
      "Epoch 7/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.2561Epoch 00006: val_loss improved from 1.48883 to 1.48354, saving model to model_weights/best_beiras_gru_checkpoint_textdata_weights.hdf5\n",
      "1056852/1056852 [==============================] - 1149s - loss: 1.2561 - val_loss: 1.4835\n",
      "Epoch 8/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.2348Epoch 00007: val_loss improved from 1.48354 to 1.48137, saving model to model_weights/best_beiras_gru_checkpoint_textdata_weights.hdf5\n",
      "1056852/1056852 [==============================] - 1150s - loss: 1.2348 - val_loss: 1.4814\n",
      "Epoch 9/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.2162Epoch 00008: val_loss improved from 1.48137 to 1.47849, saving model to model_weights/best_beiras_gru_checkpoint_textdata_weights.hdf5\n",
      "1056852/1056852 [==============================] - 1147s - loss: 1.2162 - val_loss: 1.4785\n",
      "Epoch 10/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.2000Epoch 00009: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 1148s - loss: 1.1999 - val_loss: 1.4793\n",
      "Epoch 11/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.1853Epoch 00010: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 1151s - loss: 1.1854 - val_loss: 1.4854\n",
      "Epoch 12/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.1718Epoch 00011: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 1151s - loss: 1.1717 - val_loss: 1.4818\n",
      "Epoch 13/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.1600Epoch 00012: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 1142s - loss: 1.1600 - val_loss: 1.4847\n",
      "Epoch 14/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.1493Epoch 00013: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 1147s - loss: 1.1493 - val_loss: 1.4872\n",
      "Epoch 15/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.1389Epoch 00014: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 1147s - loss: 1.1389 - val_loss: 1.4960\n",
      "Epoch 16/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.1296Epoch 00015: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 1145s - loss: 1.1296 - val_loss: 1.4932\n",
      "Epoch 17/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.1204Epoch 00016: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 1146s - loss: 1.1204 - val_loss: 1.5034\n",
      "Epoch 18/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.1130Epoch 00017: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 1147s - loss: 1.1130 - val_loss: 1.5061\n",
      "Epoch 19/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.1053Epoch 00018: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 1148s - loss: 1.1053 - val_loss: 1.5155\n",
      "Epoch 20/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.0985Epoch 00019: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 1149s - loss: 1.0985 - val_loss: 1.5182\n",
      "Epoch 21/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.0918Epoch 00020: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 1147s - loss: 1.0918 - val_loss: 1.5246\n",
      "Epoch 22/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.0863Epoch 00021: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 1147s - loss: 1.0863 - val_loss: 1.5300\n",
      "Epoch 23/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.0808Epoch 00022: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 1148s - loss: 1.0807 - val_loss: 1.5338\n",
      "Epoch 24/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.0757Epoch 00023: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 1147s - loss: 1.0757 - val_loss: 1.5361\n",
      "Epoch 25/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.0708Epoch 00024: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 1144s - loss: 1.0708 - val_loss: 1.5425\n",
      "Epoch 26/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.0661Epoch 00025: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 1153s - loss: 1.0661 - val_loss: 1.5464\n",
      "Epoch 27/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.0617Epoch 00026: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 1151s - loss: 1.0617 - val_loss: 1.5420\n",
      "Epoch 28/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.0580Epoch 00027: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 1155s - loss: 1.0580 - val_loss: 1.5504\n",
      "Epoch 29/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.0536Epoch 00028: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 1154s - loss: 1.0536 - val_loss: 1.5600\n",
      "Epoch 30/30\n",
      "1056500/1056852 [============================>.] - ETA: 0s - loss: 1.0505Epoch 00029: val_loss did not improve\n",
      "1056852/1056852 [==============================] - 1152s - loss: 1.0505 - val_loss: 1.5528\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6ed032d3c8>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### necessary functions from the keras library\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import keras\n",
    "import random\n",
    "from keras.callbacks import ModelCheckpoint   \n",
    "\n",
    "# train the model\n",
    "checkpointer = ModelCheckpoint(filepath='../model_weights/best_beiras_gru_checkpoint_textdata_weights.hdf5', verbose=1, \n",
    "                               save_best_only=True)\n",
    "# train the model\n",
    "model=create_gru_model(chars)\n",
    "model.fit(X_train, y_train, batch_size=500, epochs=30,\n",
    "           validation_data=(X_validate, y_validate),\n",
    "          callbacks=[checkpointer],verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "input chars = \n",
      "pla panfletaria contra as leoninas taxas impostas polo ministro de xustiza actual malia que vulneran\"\n",
      "\n",
      "predicted chars = \n",
      " a sua propria estrutura de constitución e a sua propria estrutura de constitución e a sua propria e\"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "poema de rosalía titulado a xusticia pola man e dado á luz no seu libro follas novas por certo que s\"\n",
      "\n",
      "predicted chars = \n",
      "e acabar en contra da sua propria estrutura de constitución e a sua propria estrutura de constitució\"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "se moito cando dixen eu que as suas políticas agresoras do común cidadán matan e a sua cospedal alcu\"\n",
      "\n",
      "predicted chars = \n",
      "ñada en contra da sua propria estrutura de constitución e a sua propria estrutura de constitución e \"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model=create_gru_model(chars)\n",
    "print_predicctions(model,'../model_weights/best_beiras_gru_checkpoint_textdata_weights.hdf5',\n",
    "                   chars_to_indices,indices_to_chars,text_clean,window_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU + Dropout\n",
    "Another type ofregularation. ** It does not work in this case **\n",
    "* GRU\n",
    "* Dropout\n",
    "* GRU\n",
    "* Dropout\n",
    "* Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense, Activation,GRU\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "def create_gru_dropout_model(chars):\n",
    "    num_chars = len(chars)\n",
    "    model= Sequential()\n",
    "    # 1 Layer .- LSTM layer 1 should be an LSTM module with 200 hidden units\n",
    "    model.add(GRU(200,input_shape = (window_size,num_chars),return_sequences=True))\n",
    "    # 2 Layer .-  Dense, with number chars unit and softmax activation\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(GRU(200))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(num_chars,activation='softmax'))\n",
    "    # initialize optimizer\n",
    "    optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "    # compile model --> make sure initialized optimizer and callbacks - as defined above - are used\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_1 (GRU)                  (None, 100, 200)          153600    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 200)          0         \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 200)               240600    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 55)                11055     \n",
      "=================================================================\n",
      "Total params: 405,255\n",
      "Trainable params: 405,255\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aind2/anaconda3/envs/beiras-rnn/lib/python3.6/site-packages/keras/models.py:848: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1174280/1174280 [==============================] - 1264s - loss: 1.9735  \n",
      "Epoch 2/30\n",
      " 458000/1174280 [==========>...................] - ETA: 772s - loss: 1.6839"
     ]
    }
   ],
   "source": [
    "\n",
    "model=create_gru_dropout_model(chars)\n",
    "model.summary()\n",
    "model.fit(X, y, batch_size=500, nb_epoch=30,verbose = 1)\n",
    "\n",
    "# save weights\n",
    "model.save_weights('../model_weights/best_beiras_gru_dropout_textdata_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "input chars = \n",
      "pla panfletaria contra as leoninas taxas impostas polo ministro de xustiza actual malia que vulneran\"\n",
      "\n",
      "predicted chars = \n",
      " por caso a contradición e de contradición e de contradición e de contradición e de contradición e d\"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "poema de rosalía titulado a xusticia pola man e dado á luz no seu libro follas novas por certo que s\"\n",
      "\n",
      "predicted chars = \n",
      "e constitue unha constitución de contradicións de competencia e a construción dos cidadáns do común \"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "se moito cando dixen eu que as suas políticas agresoras do común cidadán matan e a sua cospedal alcu\"\n",
      "\n",
      "predicted chars = \n",
      "mante en cartas de compostela a construción dos cidadáns do común de competencia e a construción dos\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model=create_gru_dropout_model(chars)\n",
    "print_predicctions(model,'../model_weights/best_beiras_gru_dropout_textdata_weights.hdf5',\n",
    "                   chars_to_indices,indices_to_chars,text_clean,window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:beiras-rnn]",
   "language": "python",
   "name": "conda-env-beiras-rnn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
